{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ELSER v2 + BM25 Search — Single-Notebook Edition\n",
        "\n",
        "**Last generated:** 2025-11-04T02:29:28\n",
        "\n",
        "**What you get**  \n",
        "- Robust hybrid search (ELSER text_expansion + BM25) with automatic fallback to BM25-only on any ML errors.  \n",
        "- Zero up‑front license/deployment probing.  \n",
        "- Easy config cell — just point to your `.xlsx`, `.xls`, or `.csv` with a text column.\n",
        "\n",
        "> **Tip:** If ELSER is not available, the notebook still works: indexing/queries will automatically fall back to BM25."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running on a fresh environment, uncomment:\n",
        "# %pip install elasticsearch=8.13.0 pandas openpyxl python-dateutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Class definition (BertDescriptionElser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Iterable, Optional, List, Sequence, Union\n",
        "\n",
        "import pandas as pd\n",
        "from dateutil import parser as dtparser\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "\n",
        "# elastic_transport.ApiError moved across versions; guard for broader compat\n",
        "try:\n",
        "    from elastic_transport import ApiError\n",
        "except Exception:\n",
        "    try:\n",
        "        from elasticsearch import ApiError  # type: ignore\n",
        "    except Exception:\n",
        "        class ApiError(Exception):\n",
        "            pass\n",
        "\n",
        "try:\n",
        "    from elasticsearch.helpers import BulkIndexError\n",
        "except Exception:\n",
        "    class BulkIndexError(Exception):\n",
        "        pass\n",
        "\n",
        "\n",
        "def _coerce_str(v) -> Optional[str]:\n",
        "    if v is None:\n",
        "        return None\n",
        "    s = str(v).strip()\n",
        "    return s if s else None\n",
        "\n",
        "\n",
        "def to_iso(v) -> Optional[str]:\n",
        "    if pd.isna(v):\n",
        "        return None\n",
        "    try:\n",
        "        if isinstance(v, datetime):\n",
        "            return v.isoformat()\n",
        "        return dtparser.parse(str(v)).isoformat()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "class BertDescriptionElser:\n",
        "    def __init__(\n",
        "        self,\n",
        "        es_url: str = \"http://localhost:9200\",\n",
        "        es_user: str = \"elastic\",\n",
        "        es_pass: str = \"changeme\",\n",
        "        index_name: str = \"chat_elser_description_only\",\n",
        "        pipeline_id: str = \"elser_v2_description_only\",\n",
        "        model_id: str = \".elser_model_2_linux-x86_64\",\n",
        "        description_col: str = \"Description\",\n",
        "        request_timeout: int = 120,\n",
        "        use_ml: bool = True,\n",
        "    ) -> None:\n",
        "        self.es = Elasticsearch(\n",
        "            es_url,\n",
        "            basic_auth=(es_user, es_pass),\n",
        "            request_timeout=request_timeout,\n",
        "            verify_certs=False,\n",
        "        )\n",
        "        self.index_name = index_name\n",
        "        self.pipeline_id = pipeline_id\n",
        "        self.model_id = model_id\n",
        "        self.description_col = description_col\n",
        "        self.use_ml_requested = use_ml  # user preference to try ELSER\n",
        "\n",
        "    # --------------------------\n",
        "    # Mapping and pipeline\n",
        "    # --------------------------\n",
        "    def ensure_index(self) -> None:\n",
        "        \"\"\"Create or update a minimal mapping. Add rank_features if we expect ML tokens.\"\"\"\n",
        "        props: Dict[str, Any] = {\n",
        "            self.description_col: {\"type\": \"text\"},\n",
        "            \"timestamp\": {\"type\": \"date\", \"ignore_malformed\": True},\n",
        "        }\n",
        "        # It is safe to declare the token field even if it won’t be used.\n",
        "        props.setdefault(\"ml\", {\"properties\": {}})\n",
        "        props[\"ml\"][\"properties\"][\"description_tokens\"] = {\"type\": \"rank_features\"}\n",
        "\n",
        "        body = {\"mappings\": {\"properties\": props}}\n",
        "        if self.es.indices.exists(index=self.index_name):\n",
        "            self.es.indices.put_mapping(index=self.index_name, properties=props)\n",
        "        else:\n",
        "            self.es.indices.create(index=self.index_name, **body)\n",
        "\n",
        "    def ensure_pipeline(self) -> None:\n",
        "        \"\"\"\n",
        "        Create or update ingest pipeline that writes to ml.description_tokens.\n",
        "        If the model is unavailable, putting this pipeline still succeeds; any error would occur at ingest-time.\n",
        "        \"\"\"\n",
        "        if not self.use_ml_requested:\n",
        "            return\n",
        "        processors = [\n",
        "            {\n",
        "                \"inference\": {\n",
        "                    \"model_id\": self.model_id,\n",
        "                    \"inference_config\": {\n",
        "                        \"text_expansion\": {\"results_field\": \"ml.description_tokens\"}\n",
        "                    },\n",
        "                    \"field_map\": {self.description_col: \"text_field\"},\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "        self.es.ingest.put_pipeline(id=self.pipeline_id, processors=processors)\n",
        "\n",
        "    def ensure_ready(self) -> None:\n",
        "        \"\"\"\n",
        "        Backward-compat shim for older scripts that call `ensure_ready()`.\n",
        "        Keep it side-effect-free for indexing; just make sure the ingest\n",
        "        pipeline exists if ML was requested. Index creation happens in\n",
        "        the caller (e.g., ensure_indexed()).\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if self.use_ml_requested:\n",
        "                self.ensure_pipeline()\n",
        "        except Exception:\n",
        "            # Don't fail here; actual search/indexing will gracefully fall back.\n",
        "            pass\n",
        "\n",
        "    # --------------------------\n",
        "    # Ingestion\n",
        "    # --------------------------\n",
        "    def _sanitize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        if self.description_col not in df.columns:\n",
        "            raise ValueError(\n",
        "                f\"Required column '{self.description_col}' not found. \"\n",
        "                f\"Available columns: {list(df.columns)}\"\n",
        "            )\n",
        "        s = df[self.description_col].astype(str).map(lambda x: x.strip())\n",
        "        df = df.copy()\n",
        "        df[self.description_col] = s\n",
        "        df = df[df[self.description_col].astype(bool)]\n",
        "        if df.empty:\n",
        "            raise ValueError(\n",
        "                f\"All rows are empty in '{self.description_col}'. Provide non-empty text.\"\n",
        "            )\n",
        "        return df\n",
        "\n",
        "    def _iter_actions(self, df: pd.DataFrame, id_field: Optional[str]) -> Iterable[Dict[str, Any]]:\n",
        "        for _, row in df.iterrows():\n",
        "            doc: Dict[str, Any] = {}\n",
        "            for c in df.columns:\n",
        "                val = row[c]\n",
        "                if pd.isna(val):\n",
        "                    continue\n",
        "                doc[c] = val\n",
        "\n",
        "            # Optional timestamp detection\n",
        "            for cand in (\"created_dttm\", \"created_at\", \"timestamp\", \"time\", \"date\"):\n",
        "                if cand in df.columns and not pd.isna(row.get(cand)):\n",
        "                    iso = to_iso(row[cand])\n",
        "                    if iso:\n",
        "                        doc[\"timestamp\"] = iso\n",
        "                        break\n",
        "\n",
        "            action = {\n",
        "                \"_op_type\": \"index\",\n",
        "                \"_index\": self.index_name,\n",
        "                \"_source\": doc,\n",
        "            }\n",
        "            if self.use_ml_requested:\n",
        "                action[\"pipeline\"] = self.pipeline_id  # safe; errors surface at bulk time\n",
        "            if id_field and id_field in row and pd.notna(row[id_field]):\n",
        "                action[\"_id\"] = str(row[id_field])\n",
        "            yield action\n",
        "\n",
        "    def bulk_index_dataframe(self, df: pd.DataFrame, id_field: Optional[str] = None, chunk_size: int = 500) -> None:\n",
        "        df = self._sanitize_dataframe(df)\n",
        "        try:\n",
        "            helpers.bulk(\n",
        "                self.es,\n",
        "                self._iter_actions(df, id_field),\n",
        "                chunk_size=chunk_size,\n",
        "                refresh=\"wait_for\",\n",
        "            )\n",
        "        except BulkIndexError as bie:\n",
        "            errors = getattr(bie, \"errors\", [])\n",
        "            preview = errors[:3]\n",
        "            raise RuntimeError(\n",
        "                f\"Bulk indexing failed for {len(errors)} documents. First errors: {preview}\"\n",
        "            ) from bie\n",
        "\n",
        "    def bulk_index_file(self, csv_or_xlsx: Union[str, Path], id_field: Optional[str] = None) -> None:\n",
        "        p = Path(csv_or_xlsx)\n",
        "        if not p.exists():\n",
        "            raise FileNotFoundError(p)\n",
        "        if p.suffix.lower() == \".csv\":\n",
        "            df = pd.read_csv(p)\n",
        "        elif p.suffix.lower() in (\".xlsx\", \".xls\"):\n",
        "            # openpyxl engine required for some environments\n",
        "            df = pd.read_excel(p, engine=\"openpyxl\")\n",
        "        else:\n",
        "            raise ValueError(\"Only .csv, .xlsx, or .xls are supported\")\n",
        "        self.bulk_index_dataframe(df, id_field=id_field)\n",
        "\n",
        "    # --------------------------\n",
        "    # Search\n",
        "    # --------------------------\n",
        "    def _build_body(self, question: str, size: int, include_elser: bool, fields_to_return: Optional[Sequence[str]]) -> Dict[str, Any]:\n",
        "        should: List[Dict[str, Any]] = []\n",
        "        # BM25 always present\n",
        "        should.append({\"match\": {self.description_col: {\"query\": question, \"boost\": 0.6}}})\n",
        "        # ELSER if requested\n",
        "        if include_elser and self.use_ml_requested:\n",
        "            should.append({\n",
        "                \"text_expansion\": {\n",
        "                    \"ml.description_tokens\": {\n",
        "                        \"model_id\": self.model_id,\n",
        "                        \"model_text\": question,\n",
        "                    }\n",
        "                }\n",
        "            })\n",
        "\n",
        "        body: Dict[str, Any] = {\n",
        "            \"size\": size,\n",
        "            \"query\": {\"bool\": {\"should\": should, \"minimum_should_match\": 1}},\n",
        "        }\n",
        "        if fields_to_return:\n",
        "            body[\"_source\"] = list(fields_to_return)\n",
        "        return body\n",
        "\n",
        "    def semantic_search(\n",
        "        self,\n",
        "        question: str,\n",
        "        size: int = 10,\n",
        "        hybrid: bool = True,\n",
        "        fields_to_return: Optional[Sequence[str]] = None,\n",
        "    ) -> pd.DataFrame:\n",
        "        if not _coerce_str(question):\n",
        "            raise ValueError(\"Provide a non-empty search question.\")\n",
        "\n",
        "        # Try ELSER + BM25 first; on API error, retry BM25-only\n",
        "        try:\n",
        "            body = self._build_body(question, size, include_elser=hybrid, fields_to_return=fields_to_return)\n",
        "            res = self.es.search(index=self.index_name, body=body)\n",
        "        except ApiError:\n",
        "            body = self._build_body(question, size, include_elser=False, fields_to_return=fields_to_return)\n",
        "            res = self.es.search(index=self.index_name, body=body)\n",
        "\n",
        "        rows: List[Dict[str, Any]] = []\n",
        "        for h in res.get(\"hits\", {}).get(\"hits\", []):\n",
        "            src = h.get(\"_source\", {})\n",
        "            rows.append({\"_score\": h.get(\"_score\", 0.0), **src})\n",
        "        return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def ensure_indexed(pipe: BertDescriptionElser, file_path: str, reindex: bool) -> None:\n",
        "    \"\"\"Create mapping/pipeline and index the provided file if requested or if index is empty.\"\"\"\n",
        "    count = 0\n",
        "    if pipe.es.indices.exists(index=pipe.index_name):\n",
        "        try:\n",
        "            count = pipe.es.count(index=pipe.index_name)[\"count\"]\n",
        "        except Exception:\n",
        "            count = 0\n",
        "\n",
        "    if reindex or count == 0:\n",
        "        if pipe.es.indices.exists(index=pipe.index_name):\n",
        "            pipe.es.indices.delete(index=pipe.index_name, ignore_unavailable=True)\n",
        "        pipe.ensure_index()\n",
        "        pipe.ensure_pipeline()  # no-op if ML unavailable\n",
        "        pipe.bulk_index_file(file_path, id_field=None)\n",
        "        count = pipe.es.count(index=pipe.index_name)[\"count\"]\n",
        "        print(f\"[INFO] Indexed docs: {count}\")\n",
        "    else:\n",
        "        print(f\"[INFO] Using existing index '{pipe.index_name}' with {count} docs.\")\n",
        "\n",
        "def do_query(pipe: BertDescriptionElser, q: str, text_col: str, size: int = 10, bm25_only: bool = False):\n",
        "    hits = pipe.semantic_search(\n",
        "        question=q,\n",
        "        size=size,\n",
        "        hybrid=(not bm25_only),  # BM25 always; add ELSER if allowed and available\n",
        "    )\n",
        "    if hits.empty:\n",
        "        print(\"(no matches)\")\n",
        "        return hits\n",
        "    # Compact view in notebooks\n",
        "    cols = [\"_score\"]\n",
        "    if text_col in hits.columns:\n",
        "        cols.append(text_col)\n",
        "    if \"timestamp\" in hits.columns:\n",
        "        cols.append(\"timestamp\")\n",
        "    try:\n",
        "        display(hits[cols] if set(cols).issubset(hits.columns) else hits)\n",
        "    except Exception:\n",
        "        print(hits[cols] if set(cols).issubset(hits.columns) else hits)\n",
        "    return hits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Configure here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Elasticsearch connection ===\n",
        "ES_URL   = \"http://localhost:9200\"\n",
        "ES_USER  = \"elastic\"\n",
        "ES_PASS  = \"changeme\"\n",
        "\n",
        "# === Data file ===  (must be .xlsx/.xls/.csv and contain the TEXT COLUMN below)\n",
        "DATA_FILE = r\"C:\\Users\\dell\\elser-python\\sample_descriptions.xlsx\"\n",
        "\n",
        "# === Index / Pipeline / Model ===\n",
        "INDEX_NAME   = \"chat_elser_description_only\"\n",
        "PIPELINE_ID  = \"elser_v2_description_only\"\n",
        "MODEL_ID     = \".elser_model_2_linux-x86_64\"\n",
        "\n",
        "# === Text column to index & search ===\n",
        "TEXT_COL = \"Description\"\n",
        "\n",
        "# === Query settings ===\n",
        "ONE_SHOT_QUERY = \"BlueSky Airlines safety compliance\"  # set None or \"\" to skip\n",
        "TOP_K          = 10\n",
        "BM25_ONLY      = False   # set True to ignore ELSER and use BM25 only\n",
        "\n",
        "# === Reindex control ===\n",
        "REINDEX = False          # set True to drop/recreate index and re-ingest the file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Preview your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA PREVIEW (first 3 rows) ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>John Doe, born on January 14, 1985, currently ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mary Johnson joined BlueSky Airlines in 2018 a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ahmed Ibrahim, a senior architect at GreenBuil...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Description\n",
              "0  John Doe, born on January 14, 1985, currently ...\n",
              "1  Mary Johnson joined BlueSky Airlines in 2018 a...\n",
              "2  Ahmed Ibrahim, a senior architect at GreenBuil..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== COLUMNS ===\n",
            "['Description']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "fp = Path(DATA_FILE)\n",
        "assert fp.exists(), f\"Input file not found: {fp}\"\n",
        "\n",
        "if fp.suffix.lower() in (\".xlsx\", \".xls\"):\n",
        "    df_preview = pd.read_excel(fp)\n",
        "elif fp.suffix.lower() == \".csv\":\n",
        "    df_preview = pd.read_csv(fp)\n",
        "else:\n",
        "    raise AssertionError(\"Only .xlsx, .xls, or .csv are supported.\")\n",
        "\n",
        "print(\"=== DATA PREVIEW (first 3 rows) ===\")\n",
        "try:\n",
        "    display(df_preview.head(3))\n",
        "except Exception:\n",
        "    print(df_preview.head(3).to_string(index=False))\n",
        "\n",
        "print(\"=== COLUMNS ===\")\n",
        "print(list(df_preview.columns))\n",
        "\n",
        "assert TEXT_COL in df_preview.columns, f\"Column '{TEXT_COL}' not found. Available: {list(df_preview.columns)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Create mapping/pipeline and index (as needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using existing index 'chat_elser_description_only' with 10 docs.\n"
          ]
        }
      ],
      "source": [
        "pipe = BertDescriptionElser(\n",
        "    es_url=ES_URL,\n",
        "    es_user=ES_USER,\n",
        "    es_pass=ES_PASS,\n",
        "    index_name=INDEX_NAME,\n",
        "    pipeline_id=PIPELINE_ID,\n",
        "    model_id=MODEL_ID,\n",
        "    description_col=TEXT_COL,\n",
        "    use_ml=(not BM25_ONLY),\n",
        ")\n",
        "\n",
        "# Back-compat shim: ensures pipeline if ML requested (safe no-op otherwise)\n",
        "pipe.ensure_ready()\n",
        "\n",
        "# Create mapping + (optional) pipeline + bulk index file (if reindex or empty)\n",
        "ensure_indexed(pipe, str(fp), reindex=REINDEX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) One-shot search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SEARCH RESULTS for: 'BlueSky Airlines safety compliance' ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_score</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.704845</td>\n",
              "      <td>Mary Johnson joined BlueSky Airlines in 2018 a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     _score                                        Description\n",
              "0  4.704845  Mary Johnson joined BlueSky Airlines in 2018 a..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if ONE_SHOT_QUERY and str(ONE_SHOT_QUERY).strip():\n",
        "    print(f\"=== SEARCH RESULTS for: {ONE_SHOT_QUERY!r} ===\")\n",
        "    _hits = do_query(pipe, ONE_SHOT_QUERY, TEXT_COL, size=TOP_K, bm25_only=BM25_ONLY)\n",
        "else:\n",
        "    print(\"(Skipped — set ONE_SHOT_QUERY to a non-empty string to run a one-shot search.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Interactive search loop (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interactive mode. Type your query and press Enter.\n",
            "Commands: :quit to exit, :help for help.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Interactive mode. Type your query and press Enter.\")\n",
        "print(\"Commands: :quit to exit, :help for help.\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        q = input(\"query> \").strip()\n",
        "    except (EOFError, KeyboardInterrupt):\n",
        "        print(\"\\nExiting.\")\n",
        "        break\n",
        "    if not q:\n",
        "        continue\n",
        "    if q in {\":quit\", \":exit\"}:\n",
        "        print(\"Exiting.\")\n",
        "        break\n",
        "    if q in {\":help\", \"help\", \"?\"}:\n",
        "        print(\"Enter any text to search. Use :quit to exit.\")\n",
        "        continue\n",
        "    print(f\"\\n=== SEARCH RESULTS for: {q!r} ===\")\n",
        "    _hits = do_query(pipe, q, TEXT_COL, size=TOP_K, bm25_only=BM25_ONLY)\n",
        "    print(\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
