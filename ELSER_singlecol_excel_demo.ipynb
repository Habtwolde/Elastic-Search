{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976dfa03",
   "metadata": {},
   "source": [
    "# ELSER: Excel (Single Column) → Elasticsearch → Semantic Search\n",
    "\n",
    "**Purpose:** Ingest a single long-text column named **`Description`** from Excel/CSV into Elasticsearch,\n",
    "generate ELSER tokens via an ingest pipeline, and run semantic search entirely on that column.\n",
    "\n",
    "### What this notebook does\n",
    "1. Connects to your local Elasticsearch (auth: `elastic/changeme`, adjustable).\n",
    "2. Ensures the ELSER v2 model is deployed and the ingest pipeline exists.\n",
    "3. Creates an index with `content` (text) and `ml.tokens` (rank_features).\n",
    "4. Reads `Description` from your sheet and indexes it as `content`, using the pipeline to create tokens.\n",
    "5. Runs a `text_expansion` query using ELSER tokens.\n",
    "\n",
    "### Requirements\n",
    "- Python packages in your venv:  \n",
    "  `pip install elasticsearch==8.14.0 \"urllib3<2\" pandas python-dateutil openpyxl`\n",
    "- ELSER v2 model `.elser_model_2_linux-x86_64` loaded and deployable (offline model already configured).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41cf0d",
   "metadata": {},
   "source": [
    "#### 0) Configure connection and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61ad56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust these for your environment\n",
    "ES_URL   = \"http://localhost:9200\"\n",
    "ES_USER  = \"elastic\"\n",
    "ES_PASS  = \"changeme\"\n",
    "\n",
    "INDEX_NAME  = \"excel_single_col\"\n",
    "FILE_PATH   = r\"C:\\Users\\dell\\elser-python\\long_distance_runners.xlsx\"  # or .csv\n",
    "SHEET_NAME  = \"Sheet1\"   # set None for CSV\n",
    "CONTENT_COL = \"Description\"  # <-- the ONLY column used for semantics\n",
    "ID_COL      = \"RowID\"        # optional; set None if not present\n",
    "UPDATED_COL = None           # optional timestamp column; set e.g. \"updated_at\"\n",
    "\n",
    "TEST_QUERY = \"who set records in long distance running?\"\n",
    "TOPK       = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37ddff",
   "metadata": {},
   "source": [
    "#### 1) Imports and ES client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7e9264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from dateutil import parser as dtparser\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "MODEL_ID     = \".elser_model_2_linux-x86_64\"\n",
    "PIPELINE_ID  = \"elser_v2_pipeline\"\n",
    "TOKENS_FIELD = \"ml.tokens\"\n",
    "\n",
    "ES = Elasticsearch(ES_URL, basic_auth=(ES_USER, ES_PASS), request_timeout=120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9f5d9",
   "metadata": {},
   "source": [
    "#### 2) Helpers: wait for ES, ensure model/pipeline/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7022e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wait_es(timeout_s: int = 60):\n",
    "    deadline = time.time() + timeout_s\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            ES.info()\n",
    "            return\n",
    "        except Exception:\n",
    "            time.sleep(1)\n",
    "    raise RuntimeError(\"Elasticsearch not responding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7dfbcf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_model_started():\n",
    "    \"\"\"Ensure the ELSER model is deployed (no-op if already started).\"\"\"\n",
    "    try:\n",
    "        stats = ES.ml.get_trained_models_stats(model_id=MODEL_ID)\n",
    "        tms = stats.get(\"trained_model_stats\", [])\n",
    "        if tms:\n",
    "            dstats = tms[0].get(\"deployment_stats\") or {}\n",
    "            if dstats.get(\"state\") == \"started\":\n",
    "                return\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        ES.ml.start_trained_model_deployment(\n",
    "            model_id=MODEL_ID,\n",
    "            number_of_allocations=1,\n",
    "            threads_per_allocation=1,\n",
    "            queue_capacity=1024,\n",
    "        )\n",
    "    except Exception:\n",
    "        # already started or transient issue—ignore for idempotence\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "101a2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_pipeline():\n",
    "    \"\"\"Create/update the ELSER ingest pipeline (idempotent).\"\"\"\n",
    "    pipeline = {\n",
    "        \"processors\": [\n",
    "            {\n",
    "                \"inference\": {\n",
    "                    \"model_id\": MODEL_ID,\n",
    "                    \"input_output\": [\n",
    "                        {\"input_field\": \"content\", \"output_field\": TOKENS_FIELD}\n",
    "                    ],\n",
    "                    \"inference_config\": {\"text_expansion\": {}}\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    ES.ingest.put_pipeline(id=PIPELINE_ID, processors=pipeline[\"processors\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c03e908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_index(es: Elasticsearch, index: str, with_extra_fields: dict | None = None):\n",
    "    \"\"\"Create an index with the minimal ELSER mapping (idempotent).\"\"\"\n",
    "    if es.indices.exists(index=index):\n",
    "        return\n",
    "    props = {\n",
    "        \"content\": {\"type\": \"text\"},\n",
    "        \"ml\": {\"properties\": {\"tokens\": {\"type\": \"rank_features\"}}}\n",
    "    }\n",
    "    if with_extra_fields:\n",
    "        props.update(with_extra_fields)\n",
    "    es.indices.create(index=index, body={\"mappings\": {\"properties\": props}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13b12b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_dt(v):\n",
    "    if pd.isna(v):\n",
    "        return None\n",
    "    if isinstance(v, datetime):\n",
    "        return v\n",
    "    try:\n",
    "        return dtparser.parse(str(v))\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6289bc7",
   "metadata": {},
   "source": [
    "#### 3) Ingest a single long-text column (`Description`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15e2455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ingest_excel_single_column(\n",
    "    es: Elasticsearch,\n",
    "    index: str,\n",
    "    file_path: Path,\n",
    "    sheet=None,\n",
    "    *,\n",
    "    content_col=\"content\",    # the ONLY column used for embeddings\n",
    "    id_col=\"id\",              # optional\n",
    "    updated_col=None,         # optional timestamp\n",
    "    batch=1000,\n",
    "):\n",
    "    file_path = Path(file_path)\n",
    "    # Load table\n",
    "    if file_path.suffix.lower() == \".xlsx\":\n",
    "        sheet_name = None\n",
    "        if sheet is not None:\n",
    "            try:\n",
    "                sheet_name = int(sheet)\n",
    "            except ValueError:\n",
    "                sheet_name = sheet\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "    elif file_path.suffix.lower() == \".csv\":\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise SystemExit(\"Unsupported tabular format. Use .xlsx or .csv\")\n",
    "\n",
    "    # Case-insensitive column mapping\n",
    "    cols = {c.lower().strip(): c for c in df.columns}\n",
    "    def col(name): return cols.get(str(name).lower(), name)\n",
    "\n",
    "    content_col = col(content_col)\n",
    "    id_col      = col(id_col) if id_col else None\n",
    "    updated_col = col(updated_col) if updated_col else None\n",
    "\n",
    "    if content_col not in df.columns:\n",
    "        raise SystemExit(f\"Missing required column '{content_col}' in {file_path.name}\")\n",
    "\n",
    "    # Ensure mapping\n",
    "    ensure_index(es, index, with_extra_fields={\n",
    "        \"id\":         {\"type\": \"keyword\"},\n",
    "        \"updated_at\": {\"type\": \"date\"}\n",
    "    })\n",
    "\n",
    "    actions = []\n",
    "    for _, row in df.iterrows():\n",
    "        rid = row.get(id_col) if id_col and id_col in df.columns else None\n",
    "        txt = row.get(content_col)\n",
    "\n",
    "        # Normalize updated_at to ISO if present\n",
    "        updated_iso = None\n",
    "        if updated_col and updated_col in df.columns:\n",
    "            try:\n",
    "                dt = to_dt(row.get(updated_col))\n",
    "                if dt is not None:\n",
    "                    updated_iso = dt.isoformat()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        doc = {\"content\": \"\" if pd.isna(txt) else str(txt)}\n",
    "        if rid is not None:\n",
    "            doc[\"id\"] = rid\n",
    "        if updated_iso is not None:\n",
    "            doc[\"updated_at\"] = updated_iso\n",
    "\n",
    "        actions.append({\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index,\n",
    "            \"_id\": str(rid) if rid is not None else None,\n",
    "            \"pipeline\": PIPELINE_ID,   # ELSER pipeline expands content -> ml.tokens\n",
    "            \"_source\": doc\n",
    "        })\n",
    "\n",
    "    if not actions:\n",
    "        print(\"No rows to index.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Indexing {len(actions)} rows from '{file_path.name}' → '{index}' via '{PIPELINE_ID}'...\")\n",
    "    success, fail = helpers.bulk(es, actions, stats_only=True, chunk_size=batch, request_timeout=120)\n",
    "    es.indices.refresh(index=index)\n",
    "    print(f\"Done. success={success}, failed={fail}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bf8db",
   "metadata": {},
   "source": [
    "#### 4) Semantic search helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18fe80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def semantic_search(index: str, query: str, size: int = 5) -> pd.DataFrame:\n",
    "    try:\n",
    "        cnt = ES.count(index=index).get(\"count\", 0)\n",
    "        if cnt == 0:\n",
    "            print(f\"(Index '{index}' has 0 docs — nothing to search yet.)\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    body = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"text_expansion\": {\n",
    "                TOKENS_FIELD: {\n",
    "                    \"model_id\": MODEL_ID,\n",
    "                    \"model_text\": query\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"id\", \"content\", \"updated_at\"]\n",
    "    }\n",
    "    res = ES.search(index=index, body=body)\n",
    "    hits = res.get(\"hits\", {}).get(\"hits\", [])\n",
    "    rows = []\n",
    "    for h in hits:\n",
    "        src = h.get(\"_source\", {})\n",
    "        rows.append({\n",
    "            \"score\": h.get(\"_score\"),\n",
    "            \"id\": src.get(\"id\"),\n",
    "            \"content_preview\": (src.get(\"content\") or \"\")[:200].replace(\"\\n\", \" \"),\n",
    "            \"updated_at\": src.get(\"updated_at\"),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b6565",
   "metadata": {},
   "source": [
    "#### 5) (Optional) Inspect your sheet columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e00e46b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in your file: ['Description']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eliud Kipchoge, a Kenyan long-distance runner,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mo Farah, originally from Somalia and represen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kenenisa Bekele of Ethiopia is a legend in lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brigid Kosgei, another Kenyan superstar, broke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Haile Gebrselassie, often considered a pioneer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description\n",
       "0  Eliud Kipchoge, a Kenyan long-distance runner,...\n",
       "1  Mo Farah, originally from Somalia and represen...\n",
       "2  Kenenisa Bekele of Ethiopia is a legend in lon...\n",
       "3  Brigid Kosgei, another Kenyan superstar, broke...\n",
       "4  Haile Gebrselassie, often considered a pioneer..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "if str(FILE_PATH).lower().endswith(\".xlsx\"):\n",
    "    df_preview = pd.read_excel(FILE_PATH, sheet_name=SHEET_NAME, engine=\"openpyxl\")\n",
    "elif str(FILE_PATH).lower().endswith(\".csv\"):\n",
    "    df_preview = pd.read_csv(FILE_PATH)\n",
    "else:\n",
    "    raise SystemExit(\"Unsupported file format; use .xlsx or .csv\")\n",
    "print(\"Columns in your file:\", list(df_preview.columns))\n",
    "df_preview.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313c181",
   "metadata": {},
   "source": [
    "#### 6) Ingest and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07712693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 5 rows from 'long_distance_runners.xlsx' → 'excel_single_col' via 'elser_v2_pipeline'...\n",
      "Done. success=5, failed=0\n",
      "Ready. Run the next cell to query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_22876\\3933649676.py:78: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  success, fail = helpers.bulk(es, actions, stats_only=True, chunk_size=batch, request_timeout=120)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wait_es()\n",
    "ensure_model_started()\n",
    "ensure_pipeline()\n",
    "\n",
    "ingest_excel_single_column(\n",
    "    ES,\n",
    "    index=INDEX_NAME,\n",
    "    file_path=FILE_PATH,\n",
    "    sheet=SHEET_NAME,\n",
    "    content_col=CONTENT_COL,\n",
    "    id_col=ID_COL,\n",
    "    updated_col=UPDATED_COL,\n",
    "    batch=1000\n",
    ")\n",
    "\n",
    "print(\"Ready. Run the next cell to query.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcb6df83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>content_preview</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.061375</td>\n",
       "      <td>None</td>\n",
       "      <td>Kenenisa Bekele of Ethiopia is a legend in lon...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.061375</td>\n",
       "      <td>None</td>\n",
       "      <td>Kenenisa Bekele of Ethiopia is a legend in lon...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.061375</td>\n",
       "      <td>None</td>\n",
       "      <td>Kenenisa Bekele of Ethiopia is a legend in lon...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.061375</td>\n",
       "      <td>None</td>\n",
       "      <td>Kenenisa Bekele of Ethiopia is a legend in lon...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.061375</td>\n",
       "      <td>None</td>\n",
       "      <td>Kenenisa Bekele of Ethiopia is a legend in lon...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score    id                                    content_preview  \\\n",
       "0  24.061375  None  Kenenisa Bekele of Ethiopia is a legend in lon...   \n",
       "1  24.061375  None  Kenenisa Bekele of Ethiopia is a legend in lon...   \n",
       "2  24.061375  None  Kenenisa Bekele of Ethiopia is a legend in lon...   \n",
       "3  24.061375  None  Kenenisa Bekele of Ethiopia is a legend in lon...   \n",
       "4  24.061375  None  Kenenisa Bekele of Ethiopia is a legend in lon...   \n",
       "\n",
       "  updated_at  \n",
       "0       None  \n",
       "1       None  \n",
       "2       None  \n",
       "3       None  \n",
       "4       None  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result_df = semantic_search(INDEX_NAME, TEST_QUERY, size=TOPK)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e3628",
   "metadata": {},
   "source": [
    "> Tip: In Kibana Dev Tools you can also run the equivalent DSL:\n",
    "\n",
    "```json\n",
    "POST excel_single_col/_search\n",
    "{\n",
    "  \"size\": 5,\n",
    "  \"query\": {\n",
    "    \"text_expansion\": {\n",
    "      \"ml.tokens\": {\n",
    "        \"model_id\": \".elser_model_2_linux-x86_64\",\n",
    "        \"model_text\": \"who set records in long distance running?\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"_source\": [\"id\",\"content\",\"updated_at\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ccdc4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fcd1cbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85be4a3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
