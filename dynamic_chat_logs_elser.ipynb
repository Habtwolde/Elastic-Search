{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42f25405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Config ---\n",
    "ES_URL  = \"http://localhost:9200\"\n",
    "ES_USER = \"elastic\"\n",
    "ES_PASS = \"changeme\"\n",
    "\n",
    "CSV_PATH      = r\"C:\\Users\\dell\\elser-python\\llm_outputs_sample.csv\"  # <- change to your CSV path\n",
    "INDEX_NAME    = \"chat_elser_dynamic\"\n",
    "PIPELINE_ID   = \"elser_v2_dynamic_pipeline\"\n",
    "MODEL_ID      = \".elser_model_2_linux-x86_64\"\n",
    "\n",
    "# Optional: cap the number of text columns to tokenize\n",
    "MAX_TEXT_COLUMNS = 16\n",
    "\n",
    "# For large CSVs, you can chunk ingest\n",
    "BULK_CHUNK_SIZE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265ed256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, time, uuid\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from dateutil import parser as dtparser\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "ES = Elasticsearch(ES_URL, basic_auth=(ES_USER, ES_PASS), request_timeout=120)\n",
    "\n",
    "def wait_es(timeout_s=60):\n",
    "    deadline = time.time() + timeout_s\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            ES.info()\n",
    "            return\n",
    "        except Exception:\n",
    "            time.sleep(1)\n",
    "    raise RuntimeError(\"Elasticsearch not responding\")\n",
    "\n",
    "def ensure_model_started(model_id: str):\n",
    "    try:\n",
    "        stats = ES.ml.get_trained_models_stats(model_id=model_id)\n",
    "        tms = stats.get(\"trained_model_stats\", [])\n",
    "        if tms and (tms[0].get(\"deployment_stats\") or {}).get(\"state\") == \"started\":\n",
    "            return\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        ES.ml.start_trained_model_deployment(\n",
    "            model_id=model_id,\n",
    "            number_of_allocations=1,\n",
    "            threads_per_allocation=1,\n",
    "            queue_capacity=1024,\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def norm_field(name: str):\n",
    "    s = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", str(name).strip())\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\").lower()\n",
    "    return s or \"field\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b76dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5  Columns: ['metadata_id', 'llm_outputs_id', 'incident_id', 'template_type', 'summary_string_llm', 'model_string', 'narrative_strings_llm', 'shipment_other_info', 'shipment_entity_info', 'created_dttm']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata_id</th>\n",
       "      <th>llm_outputs_id</th>\n",
       "      <th>incident_id</th>\n",
       "      <th>template_type</th>\n",
       "      <th>summary_string_llm</th>\n",
       "      <th>model_string</th>\n",
       "      <th>narrative_strings_llm</th>\n",
       "      <th>shipment_other_info</th>\n",
       "      <th>shipment_entity_info</th>\n",
       "      <th>created_dttm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b89d09b0-12b4-4c6b-b94c-4b3a9f8a9b60</td>\n",
       "      <td>df60a3e3-fc60-4a12-8223-cbbaea58d6ff</td>\n",
       "      <td>INC0000000001</td>\n",
       "      <td>TYPE1</td>\n",
       "      <td>Summary generated by LLM for incident 0001</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>Shipment delayed due to weather conditions.</td>\n",
       "      <td>SH123456789</td>\n",
       "      <td>FedEx Logistics</td>\n",
       "      <td>2025-10-20 09:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c84e78c3-8e9e-4a34-9f2a-202a22dbd510</td>\n",
       "      <td>1afc9e5b-5573-46a3-a012-1adfa0a4104b</td>\n",
       "      <td>INC0000000002</td>\n",
       "      <td>TYPE2</td>\n",
       "      <td>Summary generated for a mechanical failure case.</td>\n",
       "      <td>claude-3</td>\n",
       "      <td>Aircraft engine malfunction detected during ma...</td>\n",
       "      <td>SH987654321</td>\n",
       "      <td>DHL Aviation</td>\n",
       "      <td>2025-10-20 09:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f4b273cf-c4ef-4ec8-8099-567b31e0f341</td>\n",
       "      <td>afdf91f9-46b2-4a1a-bd2f-1c25e7a1a14b</td>\n",
       "      <td>INC0000000003</td>\n",
       "      <td>TYPE1</td>\n",
       "      <td>Summary created for cargo loss case.</td>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>Cargo lost during transit between hubs.</td>\n",
       "      <td>SH246810121</td>\n",
       "      <td>UPS Supply Chain</td>\n",
       "      <td>2025-10-20 09:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2e5f12df-f44d-49b2-b13a-04d4b53b0ed1</td>\n",
       "      <td>68f04df1-3e92-45c1-a66b-fb2d32c7c9e9</td>\n",
       "      <td>INC0000000004</td>\n",
       "      <td>TYPE3</td>\n",
       "      <td>Summary: customs delay incident.</td>\n",
       "      <td>llama-3</td>\n",
       "      <td>Shipment held at customs for verification.</td>\n",
       "      <td>SH135791113</td>\n",
       "      <td>USPS International</td>\n",
       "      <td>2025-10-20 09:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a7cd0a7f-482a-4d2b-9f92-05f373a09b79</td>\n",
       "      <td>cb2041a3-9a73-4d18-bba1-4419c58ff888</td>\n",
       "      <td>INC0000000005</td>\n",
       "      <td>TYPE2</td>\n",
       "      <td>Summary: package misroute resolved.</td>\n",
       "      <td>gemini-1.5</td>\n",
       "      <td>Package rerouted successfully after initial mi...</td>\n",
       "      <td>SH192837465</td>\n",
       "      <td>Amazon Logistics</td>\n",
       "      <td>2025-10-20 09:35:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            metadata_id                        llm_outputs_id  \\\n",
       "0  b89d09b0-12b4-4c6b-b94c-4b3a9f8a9b60  df60a3e3-fc60-4a12-8223-cbbaea58d6ff   \n",
       "1  c84e78c3-8e9e-4a34-9f2a-202a22dbd510  1afc9e5b-5573-46a3-a012-1adfa0a4104b   \n",
       "2  f4b273cf-c4ef-4ec8-8099-567b31e0f341  afdf91f9-46b2-4a1a-bd2f-1c25e7a1a14b   \n",
       "3  2e5f12df-f44d-49b2-b13a-04d4b53b0ed1  68f04df1-3e92-45c1-a66b-fb2d32c7c9e9   \n",
       "4  a7cd0a7f-482a-4d2b-9f92-05f373a09b79  cb2041a3-9a73-4d18-bba1-4419c58ff888   \n",
       "\n",
       "     incident_id template_type  \\\n",
       "0  INC0000000001         TYPE1   \n",
       "1  INC0000000002         TYPE2   \n",
       "2  INC0000000003         TYPE1   \n",
       "3  INC0000000004         TYPE3   \n",
       "4  INC0000000005         TYPE2   \n",
       "\n",
       "                                 summary_string_llm model_string  \\\n",
       "0        Summary generated by LLM for incident 0001        gpt-4   \n",
       "1  Summary generated for a mechanical failure case.     claude-3   \n",
       "2              Summary created for cargo loss case.   mistral-7b   \n",
       "3                  Summary: customs delay incident.      llama-3   \n",
       "4               Summary: package misroute resolved.   gemini-1.5   \n",
       "\n",
       "                               narrative_strings_llm shipment_other_info  \\\n",
       "0        Shipment delayed due to weather conditions.         SH123456789   \n",
       "1  Aircraft engine malfunction detected during ma...         SH987654321   \n",
       "2            Cargo lost during transit between hubs.         SH246810121   \n",
       "3         Shipment held at customs for verification.         SH135791113   \n",
       "4  Package rerouted successfully after initial mi...         SH192837465   \n",
       "\n",
       "  shipment_entity_info         created_dttm  \n",
       "0      FedEx Logistics  2025-10-20 09:15:00  \n",
       "1         DHL Aviation  2025-10-20 09:20:00  \n",
       "2     UPS Supply Chain  2025-10-20 09:25:00  \n",
       "3   USPS International  2025-10-20 09:30:00  \n",
       "4     Amazon Logistics  2025-10-20 09:35:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Preview CSV ---\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Rows:\", len(df), \" Columns:\", list(df.columns))\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29e8aa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected text columns: ['metadata_id', 'llm_outputs_id', 'incident_id', 'template_type', 'summary_string_llm', 'model_string', 'narrative_strings_llm', 'shipment_other_info', 'shipment_entity_info', 'created_dttm']\n",
      "Token fields to create: ['ml.metadata_id_tokens', 'ml.llm_outputs_id_tokens', 'ml.incident_id_tokens', 'ml.template_type_tokens', 'ml.summary_string_llm_tokens', 'ml.model_string_tokens', 'ml.narrative_strings_llm_tokens', 'ml.shipment_other_info_tokens', 'ml.shipment_entity_info_tokens', 'ml.created_dttm_tokens']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Detect text-like columns dynamically ---\n",
    "def is_text_series(s: pd.Series) -> bool:\n",
    "    if s.dtype == \"O\":\n",
    "        return True\n",
    "    if pd.api.types.is_numeric_dtype(s) or pd.api.types.is_datetime64_any_dtype(s):\n",
    "        return False\n",
    "    try:\n",
    "        sample = s.dropna().astype(str).head(100)\n",
    "        if len(sample) == 0:\n",
    "            return False\n",
    "        return sample.map(len).mean() >= 8\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "text_cols = [c for c in df.columns if is_text_series(df[c])]\n",
    "text_cols = text_cols[:MAX_TEXT_COLUMNS]\n",
    "print(\"Detected text columns:\", text_cols)\n",
    "\n",
    "# simple heuristic for a time field\n",
    "def find_timestamp_column(cols):\n",
    "    for name in cols:\n",
    "        if str(name).lower() in (\"timestamp\", \"created_dttm\", \"created_at\", \"time\", \"date\"):\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "timestamp_col = find_timestamp_column(df.columns)\n",
    "\n",
    "input_output = []\n",
    "dynamic_props = {}\n",
    "for col in text_cols:\n",
    "    safe = norm_field(col)\n",
    "    input_output.append({\"input_field\": col, \"output_field\": f\"ml.{safe}_tokens\"})\n",
    "    dynamic_props[col] = {\"type\": \"text\"}\n",
    "    dynamic_props.setdefault(\"ml\", {\"properties\": {}})\n",
    "    dynamic_props[\"ml\"].setdefault(\"properties\", {})[f\"{safe}_tokens\"] = {\"type\": \"rank_features\"}\n",
    "\n",
    "dynamic_props[\"content\"] = {\"type\": \"text\"}\n",
    "if timestamp_col:\n",
    "    dynamic_props[\"timestamp\"] = {\"type\": \"date\"}\n",
    "\n",
    "print(\"Token fields to create:\", [f\"ml.{norm_field(c)}_tokens\" for c in text_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a4b73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index + pipeline ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Ensure index and pipeline ---\n",
    "wait_es()\n",
    "ensure_model_started(MODEL_ID)\n",
    "\n",
    "pipeline = {\n",
    "    \"processors\": [\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": MODEL_ID,\n",
    "                \"input_output\": input_output,\n",
    "                \"inference_config\": {\"text_expansion\": {}}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "ES.ingest.put_pipeline(id=PIPELINE_ID, processors=pipeline[\"processors\"])\n",
    "\n",
    "if not ES.indices.exists(index=INDEX_NAME):\n",
    "    ES.indices.create(index=INDEX_NAME, body={\"mappings\": {\"properties\": dynamic_props}})\n",
    "else:\n",
    "    for col in text_cols:\n",
    "        safe = norm_field(col)\n",
    "        try:\n",
    "            ES.indices.put_mapping(\n",
    "                index=INDEX_NAME,\n",
    "                body={\"properties\": {\"ml\": {\"properties\": {f\"{safe}_tokens\": {\"type\": \"rank_features\"}}}}}\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"Index + pipeline ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a0d15a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 5 docs → chat_elser_dynamic via elser_v2_dynamic_pipeline ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_33980\\192585441.py:46: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  success, fail = helpers.bulk(ES, actions, stats_only=True, chunk_size=BULK_CHUNK_SIZE, request_timeout=120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk done. success= 5  failed= 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Ingest CSV rows ---\n",
    "def to_iso(v):\n",
    "    if pd.isna(v):\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(v, datetime):\n",
    "            return v.isoformat()\n",
    "        return dtparser.parse(str(v)).isoformat()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def concat_content(row, cols):\n",
    "    parts = []\n",
    "    for c in cols:\n",
    "        val = row.get(c)\n",
    "        if pd.isna(val) or val is None:\n",
    "            continue\n",
    "        txt = str(val).strip()\n",
    "        if txt:\n",
    "            parts.append(f\"{c}: {txt}\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "actions = []\n",
    "for _, row in df.iterrows():\n",
    "    doc = {}\n",
    "    for c in df.columns:\n",
    "        val = row.get(c)\n",
    "        if pd.isna(val):\n",
    "            continue\n",
    "        doc[c] = val.item() if hasattr(val, \"item\") else val\n",
    "    doc[\"content\"] = concat_content(row, text_cols)\n",
    "    if timestamp_col and timestamp_col in df.columns:\n",
    "        iso = to_iso(row.get(timestamp_col))\n",
    "        if iso:\n",
    "            doc[\"timestamp\"] = iso\n",
    "\n",
    "    actions.append({\n",
    "        \"_op_type\": \"index\",\n",
    "        \"_index\": INDEX_NAME,\n",
    "        \"_id\": str(uuid.uuid4()),\n",
    "        \"pipeline\": PIPELINE_ID,\n",
    "        \"_source\": doc\n",
    "    })\n",
    "\n",
    "print(f\"Indexing {len(actions)} docs → {INDEX_NAME} via {PIPELINE_ID} ...\")\n",
    "success, fail = helpers.bulk(ES, actions, stats_only=True, chunk_size=BULK_CHUNK_SIZE, request_timeout=120)\n",
    "ES.indices.refresh(index=INDEX_NAME)\n",
    "print(\"Bulk done. success=\", success, \" failed=\", fail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87355770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready. Example:\n",
      "semantic_search('Which responses mention high risk shipments?', size=5, hybrid=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Semantic search ---\n",
    "def build_semantic_query(model_text: str, token_fields: list[str], size=5, must_filters=None, hybrid=False):\n",
    "    should = []\n",
    "    if hybrid:\n",
    "        should.append({\n",
    "            \"multi_match\": {\n",
    "                \"query\": model_text,\n",
    "                \"fields\": [\"content^1\"]\n",
    "            }\n",
    "        })\n",
    "    for tf in token_fields:\n",
    "        should.append({\n",
    "            \"text_expansion\": {\n",
    "                tf: {\"model_id\": MODEL_ID, \"model_text\": model_text}\n",
    "            }\n",
    "        })\n",
    "    q = {\"bool\": {\"should\": should, \"minimum_should_match\": 1}}\n",
    "    if must_filters:\n",
    "        q[\"bool\"][\"filter\"] = must_filters\n",
    "    return {\"size\": size, \"query\": q, \"_source\": list(df.columns) + [\"content\", \"timestamp\"]}\n",
    "\n",
    "TOKEN_FIELDS = [f\"ml.{norm_field(c)}_tokens\" for c in text_cols]\n",
    "\n",
    "def semantic_search(query: str, size=5, hybrid=False, must_filters=None):\n",
    "    body = build_semantic_query(query, TOKEN_FIELDS, size=size, must_filters=must_filters, hybrid=hybrid)\n",
    "    res = ES.search(index=INDEX_NAME, body=body)\n",
    "    rows = []\n",
    "    for hit in res.get(\"hits\", {}).get(\"hits\", []):\n",
    "        src = hit.get(\"_source\", {})\n",
    "        rows.append({\"_score\": hit.get(\"_score\", 0.0), **{k: src.get(k) for k in (list(df.columns) + [\"content\",\"timestamp\"])}})\n",
    "    import pandas as pd\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bbe3271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: chat_elser_dynamic\n",
      "Doc count: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>metadata_id</th>\n",
       "      <th>llm_outputs_id</th>\n",
       "      <th>incident_id</th>\n",
       "      <th>template_type</th>\n",
       "      <th>summary_string_llm</th>\n",
       "      <th>model_string</th>\n",
       "      <th>narrative_strings_llm</th>\n",
       "      <th>shipment_other_info</th>\n",
       "      <th>shipment_entity_info</th>\n",
       "      <th>created_dttm</th>\n",
       "      <th>content</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.816092</td>\n",
       "      <td>b89d09b0-12b4-4c6b-b94c-4b3a9f8a9b60</td>\n",
       "      <td>df60a3e3-fc60-4a12-8223-cbbaea58d6ff</td>\n",
       "      <td>INC0000000001</td>\n",
       "      <td>TYPE1</td>\n",
       "      <td>Summary generated by LLM for incident 0001</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>Shipment delayed due to weather conditions.</td>\n",
       "      <td>SH123456789</td>\n",
       "      <td>FedEx Logistics</td>\n",
       "      <td>2025-10-20 09:15:00</td>\n",
       "      <td>metadata_id: b89d09b0-12b4-4c6b-b94c-4b3a9f8a9...</td>\n",
       "      <td>2025-10-20T09:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.152145</td>\n",
       "      <td>2e5f12df-f44d-49b2-b13a-04d4b53b0ed1</td>\n",
       "      <td>68f04df1-3e92-45c1-a66b-fb2d32c7c9e9</td>\n",
       "      <td>INC0000000004</td>\n",
       "      <td>TYPE3</td>\n",
       "      <td>Summary: customs delay incident.</td>\n",
       "      <td>llama-3</td>\n",
       "      <td>Shipment held at customs for verification.</td>\n",
       "      <td>SH135791113</td>\n",
       "      <td>USPS International</td>\n",
       "      <td>2025-10-20 09:30:00</td>\n",
       "      <td>metadata_id: 2e5f12df-f44d-49b2-b13a-04d4b53b0...</td>\n",
       "      <td>2025-10-20T09:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.626652</td>\n",
       "      <td>f4b273cf-c4ef-4ec8-8099-567b31e0f341</td>\n",
       "      <td>afdf91f9-46b2-4a1a-bd2f-1c25e7a1a14b</td>\n",
       "      <td>INC0000000003</td>\n",
       "      <td>TYPE1</td>\n",
       "      <td>Summary created for cargo loss case.</td>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>Cargo lost during transit between hubs.</td>\n",
       "      <td>SH246810121</td>\n",
       "      <td>UPS Supply Chain</td>\n",
       "      <td>2025-10-20 09:25:00</td>\n",
       "      <td>metadata_id: f4b273cf-c4ef-4ec8-8099-567b31e0f...</td>\n",
       "      <td>2025-10-20T09:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.403996</td>\n",
       "      <td>a7cd0a7f-482a-4d2b-9f92-05f373a09b79</td>\n",
       "      <td>cb2041a3-9a73-4d18-bba1-4419c58ff888</td>\n",
       "      <td>INC0000000005</td>\n",
       "      <td>TYPE2</td>\n",
       "      <td>Summary: package misroute resolved.</td>\n",
       "      <td>gemini-1.5</td>\n",
       "      <td>Package rerouted successfully after initial mi...</td>\n",
       "      <td>SH192837465</td>\n",
       "      <td>Amazon Logistics</td>\n",
       "      <td>2025-10-20 09:35:00</td>\n",
       "      <td>metadata_id: a7cd0a7f-482a-4d2b-9f92-05f373a09...</td>\n",
       "      <td>2025-10-20T09:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.509855</td>\n",
       "      <td>c84e78c3-8e9e-4a34-9f2a-202a22dbd510</td>\n",
       "      <td>1afc9e5b-5573-46a3-a012-1adfa0a4104b</td>\n",
       "      <td>INC0000000002</td>\n",
       "      <td>TYPE2</td>\n",
       "      <td>Summary generated for a mechanical failure case.</td>\n",
       "      <td>claude-3</td>\n",
       "      <td>Aircraft engine malfunction detected during ma...</td>\n",
       "      <td>SH987654321</td>\n",
       "      <td>DHL Aviation</td>\n",
       "      <td>2025-10-20 09:20:00</td>\n",
       "      <td>metadata_id: c84e78c3-8e9e-4a34-9f2a-202a22dbd...</td>\n",
       "      <td>2025-10-20T09:20:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _score                           metadata_id  \\\n",
       "0  21.816092  b89d09b0-12b4-4c6b-b94c-4b3a9f8a9b60   \n",
       "1  21.152145  2e5f12df-f44d-49b2-b13a-04d4b53b0ed1   \n",
       "2  20.626652  f4b273cf-c4ef-4ec8-8099-567b31e0f341   \n",
       "3  14.403996  a7cd0a7f-482a-4d2b-9f92-05f373a09b79   \n",
       "4   5.509855  c84e78c3-8e9e-4a34-9f2a-202a22dbd510   \n",
       "\n",
       "                         llm_outputs_id    incident_id template_type  \\\n",
       "0  df60a3e3-fc60-4a12-8223-cbbaea58d6ff  INC0000000001         TYPE1   \n",
       "1  68f04df1-3e92-45c1-a66b-fb2d32c7c9e9  INC0000000004         TYPE3   \n",
       "2  afdf91f9-46b2-4a1a-bd2f-1c25e7a1a14b  INC0000000003         TYPE1   \n",
       "3  cb2041a3-9a73-4d18-bba1-4419c58ff888  INC0000000005         TYPE2   \n",
       "4  1afc9e5b-5573-46a3-a012-1adfa0a4104b  INC0000000002         TYPE2   \n",
       "\n",
       "                                 summary_string_llm model_string  \\\n",
       "0        Summary generated by LLM for incident 0001        gpt-4   \n",
       "1                  Summary: customs delay incident.      llama-3   \n",
       "2              Summary created for cargo loss case.   mistral-7b   \n",
       "3               Summary: package misroute resolved.   gemini-1.5   \n",
       "4  Summary generated for a mechanical failure case.     claude-3   \n",
       "\n",
       "                               narrative_strings_llm shipment_other_info  \\\n",
       "0        Shipment delayed due to weather conditions.         SH123456789   \n",
       "1         Shipment held at customs for verification.         SH135791113   \n",
       "2            Cargo lost during transit between hubs.         SH246810121   \n",
       "3  Package rerouted successfully after initial mi...         SH192837465   \n",
       "4  Aircraft engine malfunction detected during ma...         SH987654321   \n",
       "\n",
       "  shipment_entity_info         created_dttm  \\\n",
       "0      FedEx Logistics  2025-10-20 09:15:00   \n",
       "1   USPS International  2025-10-20 09:30:00   \n",
       "2     UPS Supply Chain  2025-10-20 09:25:00   \n",
       "3     Amazon Logistics  2025-10-20 09:35:00   \n",
       "4         DHL Aviation  2025-10-20 09:20:00   \n",
       "\n",
       "                                             content            timestamp  \n",
       "0  metadata_id: b89d09b0-12b4-4c6b-b94c-4b3a9f8a9...  2025-10-20T09:15:00  \n",
       "1  metadata_id: 2e5f12df-f44d-49b2-b13a-04d4b53b0...  2025-10-20T09:30:00  \n",
       "2  metadata_id: f4b273cf-c4ef-4ec8-8099-567b31e0f...  2025-10-20T09:25:00  \n",
       "3  metadata_id: a7cd0a7f-482a-4d2b-9f92-05f373a09...  2025-10-20T09:35:00  \n",
       "4  metadata_id: c84e78c3-8e9e-4a34-9f2a-202a22dbd...  2025-10-20T09:20:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make sure this matches the index you ingested into earlier\n",
    "print(\"Index:\", INDEX_NAME)\n",
    "\n",
    "# Quick sanity check: do we have docs?\n",
    "try:\n",
    "    print(\"Doc count:\", ES.count(index=INDEX_NAME)[\"count\"])\n",
    "except Exception as e:\n",
    "    print(\"Count error:\", e)\n",
    "\n",
    "# Now run a semantic query and display the results\n",
    "QUESTION = \"Which responses mention high risk shipments?\"\n",
    "df_hits = semantic_search(QUESTION, size=5, hybrid=True)  # hybrid=True mixes keyword + semantic\n",
    "display(df_hits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
