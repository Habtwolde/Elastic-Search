{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f25405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "ES_URL  = \"http://localhost:9200\"\n",
    "ES_USER = \"elastic\"\n",
    "ES_PASS = \"changeme\"\n",
    "\n",
    "CSV_OR_XLSX = r\"C:\\Users\\dell\\elser-python\\sample_descriptions.xlsx\"  # <-- path to your sheet\n",
    "INDEX_NAME  = \"chat_elser_description_only\"\n",
    "PIPELINE_ID = \"elser_v2_description_only\"\n",
    "MODEL_ID    = \".elser_model_2_linux-x86_64\"\n",
    "\n",
    "DESCRIPTION_COL = \"Description\"   # <- hard requirement\n",
    "\n",
    "# --- Setup ---\n",
    "import os, time, uuid, re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from dateutil import parser as dtparser\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "ES = Elasticsearch(ES_URL, basic_auth=(ES_USER, ES_PASS), request_timeout=120)\n",
    "\n",
    "def wait_es(timeout_s=60):\n",
    "    deadline = time.time() + timeout_s\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            ES.info()\n",
    "            return\n",
    "        except Exception:\n",
    "            time.sleep(1)\n",
    "    raise RuntimeError(\"Elasticsearch not responding\")\n",
    "\n",
    "def ensure_model_started(model_id: str):\n",
    "    try:\n",
    "        stats = ES.ml.get_trained_models_stats(model_id=model_id)\n",
    "        tms = stats.get(\"trained_model_stats\", [])\n",
    "        if tms and (tms[0].get(\"deployment_stats\") or {}).get(\"state\") == \"started\":\n",
    "            return\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        ES.ml.start_trained_model_deployment(\n",
    "            model_id=model_id,\n",
    "            number_of_allocations=1,\n",
    "            threads_per_allocation=1,\n",
    "            queue_capacity=1024,\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def to_iso(v):\n",
    "    if pd.isna(v): return None\n",
    "    try:\n",
    "        if isinstance(v, datetime): return v.isoformat()\n",
    "        return dtparser.parse(str(v)).isoformat()\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ed256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline + index ready.\n"
     ]
    }
   ],
   "source": [
    "wait_es()\n",
    "ensure_model_started(MODEL_ID)\n",
    "\n",
    "# Ingest pipeline: ONLY Description -> ml.description_tokens\n",
    "pipeline = {\n",
    "    \"processors\": [\n",
    "        {\n",
    "            \"inference\": {\n",
    "                \"model_id\": MODEL_ID,\n",
    "                \"input_output\": [\n",
    "                    {\"input_field\": DESCRIPTION_COL, \"output_field\": \"ml.description_tokens\"}\n",
    "                ],\n",
    "                \"inference_config\": {\"text_expansion\": {}}\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "ES.ingest.put_pipeline(id=PIPELINE_ID, processors=pipeline[\"processors\"])\n",
    "\n",
    "# Index mapping: keep Description as text, add rank_features field\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            DESCRIPTION_COL: {\"type\": \"text\"},\n",
    "            \"ml\": {\"properties\": {\"description_tokens\": {\"type\": \"rank_features\"}}},\n",
    "            # optional timestamp if present in your CSV (e.g., created_dttm)\n",
    "            \"timestamp\": {\"type\": \"date\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if not ES.indices.exists(index=INDEX_NAME):\n",
    "    ES.indices.create(index=INDEX_NAME, body=mapping)\n",
    "else:\n",
    "    # idempotent: ensure the tokens field exists\n",
    "    ES.indices.put_mapping(index=INDEX_NAME, body={\n",
    "        \"properties\": {\"ml\": {\"properties\": {\"description_tokens\": {\"type\": \"rank_features\"}}}}\n",
    "    })\n",
    "\n",
    "print(\"Pipeline + index ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f675485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 10 docs → chat_elser_description_only via elser_v2_description_only ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\ipykernel_33644\\2936938930.py:48: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  success, fail = helpers.bulk(ES, actions, stats_only=True, chunk_size=1000, request_timeout=120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulk done. success= 10  failed= 0\n",
      "Doc count: 10\n"
     ]
    }
   ],
   "source": [
    "p = Path(CSV_OR_XLSX)\n",
    "if p.suffix.lower() == \".csv\":\n",
    "    df = pd.read_csv(p)\n",
    "elif p.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "    df = pd.read_excel(p, engine=\"openpyxl\")\n",
    "else:\n",
    "    raise SystemExit(\"Provide a .csv or .xlsx file\")\n",
    "\n",
    "# Require the Description column\n",
    "if DESCRIPTION_COL not in df.columns:\n",
    "    raise SystemExit(f\"Column '{DESCRIPTION_COL}' not found in {p.name}. Available: {list(df.columns)}\")\n",
    "\n",
    "# Try to detect a timestamp-ish column (optional)\n",
    "def find_ts(cols):\n",
    "    for c in cols:\n",
    "        if str(c).lower() in (\"created_dttm\",\"created_at\",\"timestamp\",\"time\",\"date\"):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "ts_col = find_ts(df.columns)\n",
    "\n",
    "actions = []\n",
    "for _, row in df.iterrows():\n",
    "    doc = {}\n",
    "    # Persist all columns as-is so you can still see metadata in hits\n",
    "    for c in df.columns:\n",
    "        val = row.get(c)\n",
    "        if pd.isna(val): \n",
    "            continue\n",
    "        doc[c] = val.item() if hasattr(val, \"item\") else val\n",
    "\n",
    "    # Optional normalized timestamp\n",
    "    if ts_col and ts_col in df.columns:\n",
    "        iso = to_iso(row.get(ts_col))\n",
    "        if iso:\n",
    "            doc[\"timestamp\"] = iso\n",
    "\n",
    "    # IMPORTANT: indexing runs through the pipeline that expands ONLY Description\n",
    "    actions.append({\n",
    "        \"_op_type\": \"index\",\n",
    "        \"_index\": INDEX_NAME,\n",
    "        \"_id\": str(uuid.uuid4()),\n",
    "        \"pipeline\": PIPELINE_ID,\n",
    "        \"_source\": doc\n",
    "    })\n",
    "\n",
    "print(f\"Indexing {len(actions)} docs → {INDEX_NAME} via {PIPELINE_ID} ...\")\n",
    "success, fail = helpers.bulk(ES, actions, stats_only=True, chunk_size=1000, request_timeout=120)\n",
    "ES.indices.refresh(index=INDEX_NAME)\n",
    "print(\"Bulk done. success=\", success, \" failed=\", fail)\n",
    "\n",
    "# quick count\n",
    "print(\"Doc count:\", ES.count(index=INDEX_NAME)[\"count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f2f28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.466158</td>\n",
       "      <td>Robert Green, born December 9, 1975, serves as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.147705</td>\n",
       "      <td>John Doe, born on January 14, 1985, currently ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.063447</td>\n",
       "      <td>Li Wei, born on August 5, 1993, works for Sino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.327348</td>\n",
       "      <td>Miguel Santos, born July 11, 1991, is a logist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.276300</td>\n",
       "      <td>Ahmed Ibrahim, a senior architect at GreenBuil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _score                                        Description\n",
       "0  20.466158  Robert Green, born December 9, 1975, serves as...\n",
       "1   5.147705  John Doe, born on January 14, 1985, currently ...\n",
       "2   5.063447  Li Wei, born on August 5, 1993, works for Sino...\n",
       "3   4.327348  Miguel Santos, born July 11, 1991, is a logist...\n",
       "4   4.276300  Ahmed Ibrahim, a senior architect at GreenBuil..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def search_description(question: str, size=5, hybrid=False):\n",
    "    should = []\n",
    "    if hybrid:\n",
    "        # (optional) mix in keyword search over Description too\n",
    "        should.append({\"match\": {DESCRIPTION_COL: {\"query\": question}}})\n",
    "\n",
    "    should.append({\n",
    "        \"text_expansion\": {\n",
    "            \"ml.description_tokens\": {\n",
    "                \"model_id\": MODEL_ID,\n",
    "                \"model_text\": question\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    body = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\"bool\": {\"should\": should, \"minimum_should_match\": 1}},\n",
    "        \"_source\": list(df.columns) + [DESCRIPTION_COL, \"timestamp\"]\n",
    "    }\n",
    "    res = ES.search(index=INDEX_NAME, body=body)\n",
    "    rows = []\n",
    "    for h in res.get(\"hits\", {}).get(\"hits\", []):\n",
    "        src = h.get(\"_source\", {})\n",
    "        rows.append({\"_score\": h.get(\"_score\", 0.0), DESCRIPTION_COL: src.get(DESCRIPTION_COL), **{c: src.get(c) for c in df.columns if c != DESCRIPTION_COL}})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example:\n",
    "QUESTION = \"Who works at FinEdge Capital?\"\n",
    "hits = search_description(QUESTION, size=5, hybrid=True)\n",
    "display(hits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install once per environment\n",
    "%pip install -q \"transformers>=4.44\" torch\n",
    "\n",
    "from transformers import pipeline\n",
    "import json, re\n",
    "\n",
    "ner = pipeline(\"token-classification\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "\n",
    "DATE_RE = re.compile(\n",
    "    r\"\\b(\"\n",
    "    r\"\\d{4}-\\d{2}-\\d{2}|\"\n",
    "    r\"\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\"\n",
    "    r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec|\"\n",
    "    r\"January|February|March|April|May|June|July|August|September|October|November|December)\"\n",
    "    r\"\\s+\\d{1,2},?\\s*\\d{2,4}\"\n",
    "    r\")\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_profile_bert(text: str):\n",
    "    if not text or not str(text).strip():\n",
    "        return {\"name\": None, \"employer\": None, \"dob\": None, \"raw_entities\": []}\n",
    "    ents = ner(text)\n",
    "    raw = [{\"text\": e[\"word\"], \"label\": e[\"entity_group\"]} for e in ents]\n",
    "\n",
    "    persons = [e[\"text\"] for e in raw if e[\"label\"] == \"PER\"]\n",
    "    orgs    = [e[\"text\"] for e in raw if e[\"label\"] == \"ORG\"]\n",
    "    dates   = [e[\"text\"] for e in raw if e[\"label\"] == \"DATE\"]\n",
    "\n",
    "    name = persons[0] if persons else None\n",
    "    employer = orgs[0] if orgs else None\n",
    "    dob = dates[0] if dates else None\n",
    "\n",
    "    m = DATE_RE.search(text)\n",
    "    if m: dob = dob or m.group(0)\n",
    "\n",
    "    return {\"name\": name, \"employer\": employer, \"dob\": dob, \"raw_entities\": raw}\n",
    "\n",
    "# Run NER on top hit from the last search\n",
    "if hits is None or hits.empty:\n",
    "    print(\"No hits to analyze. Run the search cell first.\")\n",
    "else:\n",
    "    top_text = hits.iloc[0][DESCRIPTION_COL]\n",
    "    profile = extract_profile_bert(top_text)\n",
    "    print(json.dumps(profile, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
