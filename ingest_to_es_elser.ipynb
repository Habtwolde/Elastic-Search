import argparse, json, math, os, sys, time, uuid
from pathlib import Path
from datetime import datetime

import pandas as pd
from dateutil import parser as dtparser
import fitz  # PyMuPDF
from elasticsearch import Elasticsearch, helpers

# ---- Common ES/ELSER config ----
ES_URL   = os.environ.get("ES_URL", "http://localhost:9200")
ES_USER  = os.environ.get("ES_USER", "elastic")
ES_PASS  = os.environ.get("ES_PASS", "changeme")

MODEL_ID     = ".elser_model_2_linux-x86_64"
PIPELINE_ID  = "elser_v2_pipeline"
TOKENS_FIELD = "ml.tokens"

# ---- Connect ----
ES = Elasticsearch(ES_URL, basic_auth=(ES_USER, ES_PASS), request_timeout=120)

def wait_es(timeout_s=60):
    deadline = time.time() + timeout_s
    while time.time() < deadline:
        try:
            ES.info()
            return
        except Exception:
            time.sleep(1)
    raise RuntimeError("Elasticsearch not responding")

def ensure_model_started():
    """Verify the ELSER model is deployed; start if not."""
    try:
        stats = ES.ml.get_trained_models_stats(model_id=MODEL_ID)
        tms = stats.get("trained_model_stats", [])
        if tms:
            dstats = tms[0].get("deployment_stats") or {}
            if dstats.get("state") == "started":
                return
    except Exception:
        pass
    try:
        ES.ml.start_trained_model_deployment(
            model_id=MODEL_ID,
            number_of_allocations=1,
            threads_per_allocation=1,
            queue_capacity=1024,
        )
    except Exception:
        # no-op if already started / transient
        pass

def ensure_pipeline():
    """Create/update the ELSER pipeline (idempotent)."""
    pipeline = {
        "processors": [
            {
                "inference": {
                    "model_id": MODEL_ID,
                    "input_output": [
                        {"input_field": "content", "output_field": TOKENS_FIELD}
                    ],
                    "inference_config": {"text_expansion": {}}
                }
            }
        ]
    }
    ES.ingest.put_pipeline(id=PIPELINE_ID, processors=pipeline["processors"])

def ensure_index(index: str, with_extra_fields=None):
    """Create index with ELSER mapping if missing."""
    if ES.indices.exists(index=index):
        return
    props = {
        "content": {"type": "text"},
        "ml": {"properties": {"tokens": {"type": "rank_features"}}}
    }
    if with_extra_fields:
        props.update(with_extra_fields)
    ES.indices.create(index=index, body={"mappings": {"properties": props}})

# ---------- helpers ----------
def to_dt(v):
    if pd.isna(v):
        return None
    if isinstance(v, datetime):
        return v
    try:
        return dtparser.parse(str(v))
    except Exception:
        return None

# ---------- Excel / CSV ingestion ----------
def ingest_table(
    index: str,
    file_path: Path,
    sheet=None,
    *,
    id_col="id",
    title_col="title",
    body_col="body",
    updated_col="updated_at",
    country_col="Country",
    date_col="Date",
    time_col="Time (HH:MM:SS)",
    position_col="Position",
    batch=1000,
):
    # read df
    if file_path.suffix.lower() == ".xlsx":
        sheet_name = None
        if sheet is not None:
            try:
                sheet_name = int(sheet)
            except ValueError:
                sheet_name = sheet
        df = pd.read_excel(file_path, sheet_name=sheet_name, engine="openpyxl")
    elif file_path.suffix.lower() == ".csv":
        df = pd.read_csv(file_path)
    else:
        raise SystemExit("Unsupported tabular format. Use .xlsx or .csv")

    # case-insensitive column mapping
    cols = {c.lower().strip(): c for c in df.columns}
    def col(name): return cols.get(name.lower(), name)

    id_col       = col(id_col)
    title_col    = col(title_col)   # Name
    body_col     = col(body_col)    # Event (by your CLI)
    updated_col  = col(updated_col)
    country_col  = col(country_col)
    date_col     = col(date_col)
    time_col     = col(time_col)
    position_col = col(position_col)

    missing = [c for c in [id_col, title_col] if c not in df.columns]
    if missing:
        print(f"ERROR: Missing required columns: {missing}", file=sys.stderr)
        sys.exit(2)

    actions = []
    for _, row in df.iterrows():
        rid      = row.get(id_col)
        title    = row.get(title_col)        # Name
        event    = row.get(body_col) if body_col in df.columns else None
        country  = row.get(country_col) if country_col in df.columns else None
        date_val = row.get(date_col) if date_col in df.columns else None
        time_val = row.get(time_col) if time_col in df.columns else None
        pos_val  = row.get(position_col) if position_col in df.columns else None
        updated  = to_dt(row.get(updated_col)) if updated_col in df.columns else None

        # Normalize date to ISO string if present
        date_iso = None
        if date_val is not None:
            try:
                date_iso = to_dt(date_val).date().isoformat()
            except Exception:
                pass

        # Rich content text for ELSER
        parts = []
        if title:    parts.append(str(title))
        if event:    parts.append(f"Event: {event}")
        if country:  parts.append(f"Country: {country}")
        if date_iso: parts.append(f"Date: {date_iso}")
        if time_val is not None and str(time_val).strip() != "":
            parts.append(f"Time: {time_val}")
        if pos_val is not None and str(pos_val).strip() != "":
            parts.append(f"Position: {pos_val}")

        content = ". ".join(parts) if parts else (str(title) or "")

        # Document to index (kept structured fields too)
        doc = {
            "id":       rid,
            "title":    title,          # Name
            "body":     event,          # Event (compat)
            "event":    event,
            "country":  country,
            "date":     date_iso,
            "time_raw": str(time_val) if time_val is not None else None,
            "position": int(pos_val) if str(pos_val).isdigit() else None,
            "content":  content,
        }
        if updated is not None:
            doc["updated_at"] = updated.isoformat()

        actions.append({
            "_op_type": "index",
            "_index": index,
            "_id": str(rid) if rid is not None else None,
            "pipeline": PIPELINE_ID,
            "_source": doc
        })

    if not actions:
        print("No rows to index.")
        return

    print(f"Indexing {len(actions)} rows from '{file_path.name}' → '{index}' via '{PIPELINE_ID}'...")
    success, fail = helpers.bulk(ES, actions, stats_only=True, chunk_size=batch, request_timeout=120)
    ES.indices.refresh(index=index)
    print(f"Done. success={success}, failed={fail}")

# ---------- PDF ingestion ----------
def chunk_text(text: str, chunk_size: int = 1200, overlap: int = 200):
    text = (text or "").strip()
    if not text:
        return []
    chunks = []
    n = len(text)
    start = 0
    while start < n:
        end = min(n, start + chunk_size)
        slice_ = text[start:end]
        if end < n:
            last_dot = slice_.rfind(".")
            if last_dot > int(chunk_size * 0.6):
                end = start + last_dot + 1
                slice_ = text[start:end]
        chunk = slice_.strip()
        if chunk:
            chunks.append(chunk)
        start = end if end >= n else end - overlap
    return chunks

def extract_pdf(path: Path, max_pages: int | None = None):
    with fitz.open(path) as doc:
        total = doc.page_count
        limit = total if max_pages is None else min(max_pages, total)
        for i in range(limit):
            page = doc.load_page(i)
            txt = page.get_text("text") or ""
            if txt.strip():
                yield (i + 1, txt)

def bulk_line(index: str, pdf_path: Path, page: int, chunk_id: int, content: str):
    meta = {"index": {"_index": index, "_id": str(uuid.uuid4())}}
    src = {"path": str(pdf_path), "page": page, "chunk": chunk_id, "content": content}
    return json.dumps(meta) + "\n" + json.dumps(src, ensure_ascii=False) + "\n"

def ingest_pdf(index: str, input_path: Path, chunk_size=1200, overlap=200, max_pages=None, batch=500):
    pdfs = [input_path] if input_path.is_file() else [p for p in input_path.rglob("*.pdf")]
    if not pdfs:
        print("No PDFs found.")
        return
    print(f"Indexing {len(pdfs)} PDF(s) → '{index}' via '{PIPELINE_ID}'...")
    buf, sent = [], 0
    for pdf in pdfs:
        try:
            for page_num, text in extract_pdf(pdf, max_pages=max_pages):
                for j, chunk in enumerate(chunk_text(text, chunk_size, overlap), start=1):
                    buf.append(bulk_line(index, pdf, page_num, j, chunk))
                    if len(buf) >= batch * 2:  # 2 lines per action
                        resp = ES.bulk(body="".join(buf), pipeline=PIPELINE_ID)
                        if resp.get("errors"):
                            first = next((it for it in resp["items"] if "error" in list(it.values())[0]), None)
                            print("Bulk errors; first:", first)
                        sent += len(buf) // 2
                        buf = []
        except Exception as e:
            print(f"Error processing {pdf}: {e}")
    if buf:
        resp = ES.bulk(body="".join(buf), pipeline=PIPELINE_ID)
        if resp.get("errors"):
            first = next((it for it in resp["items"] if "error" in list(it.values())[0]), None)
            print("Bulk errors; first:", first)
        sent += len(buf) // 2
    ES.indices.refresh(index=index)
    print(f"Done. Indexed ~{int(sent)} chunks.")

# ---------- Query ----------
def semantic_search(index: str, query: str, size: int = 5):
    try:
        cnt = ES.count(index=index).get("count", 0)
        if cnt == 0:
            print(f"(Index '{index}' has 0 docs — nothing to search yet.)")
    except Exception:
        pass

    body = {
        "size": size,
        "query": {
            "text_expansion": {
                TOKENS_FIELD: {
                    "model_id": MODEL_ID,
                    "model_text": query
                }
            }
        },
        "_source": ["id","title","event","country","content","path","page","updated_at"]
    }
    res = ES.search(index=index, body=body)
    print(f"\nQuery: {query}")
    hits = res.get("hits", {}).get("hits", [])
    if not hits:
        print("- no hits -")
        return res
    for h in hits:
        src = h["_source"]
        title = src.get("title") or Path(src.get("path","")).name or ""
        preview = (src.get("content") or "")[:160].replace("\n"," ")
        print(f"- score={h['_score']:.3f}  title={title}  page={src.get('page')}  preview={preview}")
    return res

# ---------- CLI ----------
def main():
    ap = argparse.ArgumentParser(
        description="Ingest Excel/CSV/PDF into Elasticsearch with ELSER pipeline (ml.tokens) and optionally query."
    )
    ap.add_argument("--index", required=True, help="Target index (e.g., oracle_elser_index or pdf_elser_index)")
    ap.add_argument("--file", required=True, help="Path to .xlsx / .csv / .pdf or a folder of PDFs")
    ap.add_argument("--sheet", default=None, help="Excel sheet name or index (for .xlsx)")
    ap.add_argument("--id-col", default="id")
    ap.add_argument("--title-col", default="title")
    ap.add_argument("--body-col", default="body")
    ap.add_argument("--updated-col", default="updated_at")
    ap.add_argument("--country-col",  default="Country")
    ap.add_argument("--date-col",     default="Date")
    ap.add_argument("--time-col",     default="Time (HH:MM:SS)")
    ap.add_argument("--position-col", default="Position")
    ap.add_argument("--batch", type=int, default=1000)
    ap.add_argument("--chunk-size", type=int, default=1200, help="PDF chars per chunk")
    ap.add_argument("--overlap", type=int, default=200, help="PDF chunk overlap")
    ap.add_argument("--max-pages", type=int, default=None, help="PDF page limit per doc")
    ap.add_argument("--query", default=None, help="Optional: run a semantic question after ingest")
    ap.add_argument("--topk", type=int, default=5, help="Hits to return for --query")

    args = ap.parse_args()

    wait_es()
    ensure_model_started()
    ensure_pipeline()

    p = Path(args.file)

    if p.suffix.lower() in [".xlsx", ".csv"]:
        # table-like index (id/title/body/content/updated_at + extras)
        ensure_index(args.index, with_extra_fields={
            "id":        {"type": "keyword"},
            "title":     {"type": "text"},
            "body":      {"type": "text"},
            "country":   {"type": "keyword"},
            "event":     {"type": "keyword"},
            "date":      {"type": "date"},
            "time_raw":  {"type": "keyword"},
            "position":  {"type": "integer"},
            "updated_at":{"type": "date"}
        })
        ingest_table(
            index=args.index,
            file_path=p,
            sheet=args.sheet,
            id_col=args.id_col,
            title_col=args.title_col,
            body_col=args.body_col,
            updated_col=args.updated_col,
            country_col=args.country_col,
            date_col=args.date_col,
            time_col=args.time_col,
            position_col=args.position_col,
            batch=args.batch
        )
    else:
        # PDF mode: support single file or folder
        ensure_index(args.index, with_extra_fields={
            "path": {"type": "keyword"},
            "page": {"type": "integer"},
            "chunk": {"type": "integer"},
        })
        ingest_pdf(
            index=args.index,
            input_path=p,
            chunk_size=args.chunk_size,
            overlap=args.overlap,
            max_pages=args.max_pages,
            batch=args.batch
        )

    if args.query:
        semantic_search(args.index, args.query, size=args.topk)

if __name__ == "__main__":
    main()
