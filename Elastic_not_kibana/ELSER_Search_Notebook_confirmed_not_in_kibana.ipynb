{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ELSER v2 + BM25 Search — Single-Notebook Edition ES 9\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install \"elasticsearch>=9.1,<9.2\" pandas openpyxl python-dateutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Class definition (BertDescriptionElser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UPDATED: ES 9.x endpoint-based ELSER (no Kibana, no old text_expansion processor)\n",
        "from __future__ import annotations\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Iterable, Optional, List, Sequence, Union\n",
        "import json\n",
        "import pandas as pd\n",
        "from dateutil import parser as dtparser\n",
        "from elasticsearch import Elasticsearch, helpers\n",
        "try:\n",
        "    from elastic_transport import ApiError\n",
        "except Exception:\n",
        "    try:\n",
        "        from elasticsearch import ApiError  # type: ignore\n",
        "    except Exception:\n",
        "        class ApiError(Exception):\n",
        "            pass\n",
        "try:\n",
        "    from elasticsearch.helpers import BulkIndexError\n",
        "except Exception:\n",
        "    class BulkIndexError(Exception):\n",
        "        pass\n",
        "\n",
        "def _coerce_str(v) -> Optional[str]:\n",
        "    if v is None:\n",
        "        return None\n",
        "    s = str(v).strip()\n",
        "    return s if s else None\n",
        "\n",
        "def to_iso(v) -> Optional[str]:\n",
        "    if pd.isna(v):\n",
        "        return None\n",
        "    try:\n",
        "        if isinstance(v, datetime):\n",
        "            return v.isoformat()\n",
        "        return dtparser.parse(str(v)).isoformat()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "class BertDescriptionElser:\n",
        "    \"\"\"\n",
        "    ES 9.x implementation:\n",
        "      - Stores doc text in <description_col>\n",
        "      - Stores ELSER sparse vector tokens in ml.description_tokens (type=sparse_vector)\n",
        "      - At query time, uses sparse_vector query with inference_id=<endpoint_id> (hybrid with BM25)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        es_url: str = \"http://localhost:9200\",\n",
        "        es_user: str = \"elastic\",\n",
        "        es_pass: str = \"changeme\",\n",
        "        index_name: str = \"chat_elser_description_only\",\n",
        "        endpoint_id: str = \"elser-local\",       # <-- NEW: inference endpoint\n",
        "        description_col: str = \"Description\",\n",
        "        request_timeout: int = 120,\n",
        "        use_ml: bool = True,                   # toggle ELSER usage\n",
        "    ) -> None:\n",
        "        self.es = Elasticsearch(\n",
        "            es_url,\n",
        "            basic_auth=(es_user, es_pass),\n",
        "            request_timeout=request_timeout,\n",
        "            verify_certs=False,\n",
        "        )\n",
        "        self.index_name = index_name\n",
        "        self.endpoint_id = endpoint_id\n",
        "        self.description_col = description_col\n",
        "        self.use_ml_requested = use_ml\n",
        "        self.es_url = es_url  # for direct HTTP calls if needed\n",
        "\n",
        "    # ---------- Index + Mapping ----------\n",
        "    def ensure_index(self) -> None:\n",
        "        \"\"\"\n",
        "        Creates mapping with sparse_vector at ml.description_tokens.\n",
        "        Safe to call multiple times.\n",
        "        \"\"\"\n",
        "        props: Dict[str, Any] = {\n",
        "            self.description_col: {\"type\": \"text\"},\n",
        "            \"timestamp\": {\"type\": \"date\", \"ignore_malformed\": True},\n",
        "            \"ml\": {\n",
        "                \"properties\": {\n",
        "                    \"description_tokens\": {\"type\": \"sparse_vector\"}  # <-- ES 9.x sparse vector\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        body = {\"mappings\": {\"properties\": props}}\n",
        "        if self.es.indices.exists(index=self.index_name):\n",
        "            self.es.indices.put_mapping(index=self.index_name, properties=props)\n",
        "        else:\n",
        "            self.es.indices.create(index=self.index_name, **body)\n",
        "\n",
        "    # ---------- Inference (ELSER) ----------\n",
        "    def _infer_sparse(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calls POST /_inference/sparse_embedding/{endpoint_id} with { \"input\": text }\n",
        "        Returns token->weight dict suitable for sparse_vector field.\n",
        "        \"\"\"\n",
        "        # use elasticsearch client’s transport to POST arbitrary path\n",
        "        path = f\"/_inference/sparse_embedding/{self.endpoint_id}\"\n",
        "        resp = self.es.perform_request(\n",
        "            method=\"POST\",\n",
        "            path=path,\n",
        "            body={\"input\": text},\n",
        "        )\n",
        "        # shape: { \"sparse_embedding\": [ { \"is_truncated\": bool, \"embedding\": {token: weight, ...} } ] }\n",
        "        emb = resp[\"sparse_embedding\"][0][\"embedding\"]\n",
        "        return emb\n",
        "\n",
        "    # ---------- Data prep ----------\n",
        "    def _sanitize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        if self.description_col not in df.columns:\n",
        "            raise ValueError(\n",
        "                f\"Required column '{self.description_col}' not found. Available columns: {list(df.columns)}\"\n",
        "            )\n",
        "        s = df[self.description_col].astype(str).map(lambda x: x.strip())\n",
        "        df = df.copy()\n",
        "        df[self.description_col] = s\n",
        "        df = df[df[self.description_col].astype(bool)]\n",
        "        if df.empty:\n",
        "            raise ValueError(\n",
        "                f\"All rows are empty in '{self.description_col}'. Provide non-empty text.\"\n",
        "            )\n",
        "        return df\n",
        "\n",
        "    # ---------- Bulk index ----------\n",
        "    def _iter_actions(self, df: pd.DataFrame, id_field: Optional[str]) -> Iterable[Dict[str, Any]]:\n",
        "        for _, row in df.iterrows():\n",
        "            doc: Dict[str, Any] = {}\n",
        "            for c in df.columns:\n",
        "                val = row[c]\n",
        "                if pd.isna(val):\n",
        "                    continue\n",
        "                doc[c] = val\n",
        "\n",
        "            # set timestamp if found\n",
        "            for cand in (\"created_dttm\", \"created_at\", \"timestamp\", \"time\", \"date\"):\n",
        "                if cand in df.columns and not pd.isna(row.get(cand)):\n",
        "                    iso = to_iso(row[cand])\n",
        "                    if iso:\n",
        "                        doc[\"timestamp\"] = iso\n",
        "                        break\n",
        "\n",
        "            # add ELSER sparse vector\n",
        "            if self.use_ml_requested:\n",
        "                try:\n",
        "                    tokens = self._infer_sparse(str(doc[self.description_col]))\n",
        "                    doc.setdefault(\"ml\", {})[\"description_tokens\"] = tokens\n",
        "                except Exception as e:\n",
        "                    # if inference fails, index without tokens\n",
        "                    pass\n",
        "\n",
        "            action = {\"_op_type\": \"index\", \"_index\": self.index_name, \"_source\": doc}\n",
        "            if id_field and id_field in row and pd.notna(row[id_field]):\n",
        "                action[\"_id\"] = str(row[id_field])\n",
        "            yield action\n",
        "\n",
        "    def bulk_index_dataframe(self, df: pd.DataFrame, id_field: Optional[str] = None, chunk_size: int = 300) -> None:\n",
        "        df = self._sanitize_dataframe(df)\n",
        "        try:\n",
        "            helpers.bulk(\n",
        "                self.es,\n",
        "                self._iter_actions(df, id_field),\n",
        "                chunk_size=chunk_size,\n",
        "                refresh=\"wait_for\",\n",
        "            )\n",
        "        except BulkIndexError as bie:\n",
        "            errors = getattr(bie, \"errors\", [])\n",
        "            preview = errors[:3]\n",
        "            raise RuntimeError(\n",
        "                f\"Bulk indexing failed for {len(errors)} documents. First errors: {preview}\"\n",
        "            ) from bie\n",
        "\n",
        "    def bulk_index_file(self, csv_or_xlsx: Union[str, Path], id_field: Optional[str] = None) -> None:\n",
        "        p = Path(csv_or_xlsx)\n",
        "        if not p.exists():\n",
        "            raise FileNotFoundError(p)\n",
        "        if p.suffix.lower() == \".csv\":\n",
        "            df = pd.read_csv(p)\n",
        "        elif p.suffix.lower() in (\".xlsx\", \".xls\"):\n",
        "            df = pd.read_excel(p, engine=\"openpyxl\")\n",
        "        else:\n",
        "            raise ValueError(\"Only .csv, .xlsx, or .xls are supported\")\n",
        "        self.bulk_index_dataframe(df, id_field=id_field)\n",
        "\n",
        "    # ---------- Query ----------\n",
        "    def _build_body(self, question: str, size: int, include_elser: bool, fields_to_return: Optional[Sequence[str]]) -> Dict[str, Any]:\n",
        "        should: List[Dict[str, Any]] = []\n",
        "        # BM25\n",
        "        should.append({\"match\": {self.description_col: {\"query\": question, \"boost\": 0.6}}})\n",
        "        # ELSER sparse query (endpoint at query time)\n",
        "        if include_elser and self.use_ml_requested:\n",
        "            should.append({\n",
        "                \"sparse_vector\": {\n",
        "                    \"field\": \"ml.description_tokens\",\n",
        "                    \"inference_id\": self.endpoint_id,\n",
        "                    \"query\": question\n",
        "                }\n",
        "            })\n",
        "        body: Dict[str, Any] = {\"size\": size, \"query\": {\"bool\": {\"should\": should, \"minimum_should_match\": 1}}}\n",
        "        if fields_to_return:\n",
        "            body[\"_source\"] = list(fields_to_return)\n",
        "        return body\n",
        "\n",
        "    def semantic_search(self, question: str, size: int = 10, hybrid: bool = True, fields_to_return: Optional[Sequence[str]] = None) -> pd.DataFrame:\n",
        "        if not _coerce_str(question):\n",
        "            raise ValueError(\"Provide a non-empty search question.\")\n",
        "        try:\n",
        "            body = self._build_body(question, size, include_elser=hybrid, fields_to_return=fields_to_return)\n",
        "            res = self.es.search(index=self.index_name, body=body)\n",
        "        except ApiError:\n",
        "            # fallback BM25-only\n",
        "            body = self._build_body(question, size, include_elser=False, fields_to_return=fields_to_return)\n",
        "            res = self.es.search(index=self.index_name, body=body)\n",
        "        rows: List[Dict[str, Any]] = []\n",
        "        for h in res.get(\"hits\", {}).get(\"hits\", []):\n",
        "            src = h.get(\"_source\", {})\n",
        "            rows.append({\"_score\": h.get(\"_score\", 0.0), **src})\n",
        "        return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UPDATED: no ingest pipeline; we index tokens client-side\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def ensure_indexed(pipe: BertDescriptionElser, file_path: str, reindex: bool) -> None:\n",
        "    count = 0\n",
        "    if pipe.es.indices.exists(index=pipe.index_name):\n",
        "        try:\n",
        "            count = pipe.es.count(index=pipe.index_name)[\"count\"]\n",
        "        except Exception:\n",
        "            count = 0\n",
        "    if reindex or count == 0:\n",
        "        if pipe.es.indices.exists(index=pipe.index_name):\n",
        "            pipe.es.indices.delete(index=pipe.index_name, ignore_unavailable=True)\n",
        "        pipe.ensure_index()\n",
        "        pipe.bulk_index_file(file_path, id_field=None)   # tokens added client-side via endpoint\n",
        "        count = pipe.es.count(index=pipe.index_name)[\"count\"]\n",
        "        print(f\"[INFO] Indexed docs: {count}\")\n",
        "    else:\n",
        "        print(f\"[INFO] Using existing index '{pipe.index_name}' with {count} docs.\")\n",
        "\n",
        "def do_query(pipe: BertDescriptionElser, q: str, text_col: str, size: int = 10, bm25_only: bool = False):\n",
        "    hits = pipe.semantic_search(question=q, size=size, hybrid=(not bm25_only))\n",
        "    if hits.empty:\n",
        "        print(\"(no matches)\")\n",
        "        return hits\n",
        "    cols = [\"_score\"]\n",
        "    if text_col in hits.columns:\n",
        "        cols.append(text_col)\n",
        "    if \"timestamp\" in hits.columns:\n",
        "        cols.append(\"timestamp\")\n",
        "    try:\n",
        "        display(hits[cols] if set(cols).issubset(hits.columns) else hits)\n",
        "    except Exception:\n",
        "        print(hits[cols] if set(cols).issubset(hits.columns) else hits)\n",
        "    return hits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Configure here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UPDATED: use endpoint id; model id not needed here\n",
        "ES_URL    = \"http://localhost:9200\"\n",
        "ES_USER   = \"elastic\"\n",
        "ES_PASS   = \"changeme\"\n",
        "\n",
        "DATA_FILE   = r\"C:\\Users\\dell\\elser-python\\Other\\sample_descriptions.xlsx\"\n",
        "INDEX_NAME  = \"chat_elser_description_only\"\n",
        "ENDPOINT_ID = \"elser-local\"   # <-- must exist (we created this earlier)\n",
        "\n",
        "TEXT_COL       = \"Description\"\n",
        "ONE_SHOT_QUERY = \"BlueSky Airlines safety compliance\"\n",
        "TOP_K          = 10\n",
        "BM25_ONLY      = False\n",
        "REINDEX        = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4b) Verify server and model, auto-switch BM25 if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Server version: 9.1.3\n",
            "[WARN] infer() test failed: BadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Accept, Content-Type]', A compatible version is required on both Content-Type and Accept headers if either one has requested a compatible version. Accept=null, Content-Type=application/vnd.elasticsearch+json; compatible-with=9)\n",
            "[INFO] BM25-only mode\n"
          ]
        }
      ],
      "source": [
        "# UPDATED: correct header for ES 9.x infer call\n",
        "from elasticsearch import Elasticsearch\n",
        "\n",
        "def _parse_major(ver: str):\n",
        "    try:\n",
        "        return int(str(ver).split(\".\")[0])\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "es_check = Elasticsearch(\n",
        "    ES_URL,\n",
        "    basic_auth=(ES_USER, ES_PASS),\n",
        "    verify_certs=False,\n",
        "    request_timeout=60\n",
        ")\n",
        "\n",
        "try:\n",
        "    server_info = es_check.info()\n",
        "except Exception as e:\n",
        "    raise SystemExit(f\"[FATAL] Could not connect to Elasticsearch at {ES_URL}: {e}\")\n",
        "\n",
        "server_ver = server_info.get(\"version\", {}).get(\"number\")\n",
        "major = _parse_major(server_ver)\n",
        "print(f\"[INFO] Server version: {server_ver}\")\n",
        "if major != 9:\n",
        "    print(\"[WARN] Server is not 9.x. Pin the Python client accordingly.\")\n",
        "\n",
        "def endpoint_exists(es: Elasticsearch, endpoint_id: str) -> bool:\n",
        "    try:\n",
        "        es.perform_request(\"GET\", f\"/_inference/sparse_embedding/{endpoint_id}\")\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "BM25_ONLY = bool(BM25_ONLY)\n",
        "if endpoint_exists(es_check, ENDPOINT_ID):\n",
        "    try:\n",
        "        resp = es_check.perform_request(\n",
        "            method=\"POST\",\n",
        "            path=f\"/_inference/sparse_embedding/{ENDPOINT_ID}\",\n",
        "            headers={\"Content-Type\": \"application/json\"},    # <-- FIXED\n",
        "            body={\"input\": \"hello world\"},\n",
        "        )\n",
        "        print(\"[INFO] infer() smoke test: OK (endpoint responding).\")\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] infer() test failed: {e}\")\n",
        "        BM25_ONLY = True\n",
        "else:\n",
        "    print(f\"[WARN] Inference endpoint {ENDPOINT_ID!r} not found.\")\n",
        "    BM25_ONLY = True\n",
        "\n",
        "print(\"[INFO] Hybrid allowed\" if not BM25_ONLY else \"[INFO] BM25-only mode\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5) Preview your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA PREVIEW (first 3 rows) ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>John Doe, born on January 14, 1985, currently ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mary Johnson joined BlueSky Airlines in 2018 a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ahmed Ibrahim, a senior architect at GreenBuil...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Description\n",
              "0  John Doe, born on January 14, 1985, currently ...\n",
              "1  Mary Johnson joined BlueSky Airlines in 2018 a...\n",
              "2  Ahmed Ibrahim, a senior architect at GreenBuil..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== COLUMNS ===\n",
            "['Description']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "fp = Path(DATA_FILE)\n",
        "assert fp.exists(), f\"Input file not found: {fp}\"\n",
        "if fp.suffix.lower() in (\".xlsx\", \".xls\"):\n",
        "    df_preview = pd.read_excel(fp)\n",
        "elif fp.suffix.lower() == \".csv\":\n",
        "    df_preview = pd.read_csv(fp)\n",
        "else:\n",
        "    raise AssertionError(\"Only .xlsx, .xls, or .csv are supported.\")\n",
        "print(\"=== DATA PREVIEW (first 3 rows) ===\")\n",
        "try:\n",
        "    display(df_preview.head(3))\n",
        "except Exception:\n",
        "    print(df_preview.head(3).to_string(index=False))\n",
        "print(\"=== COLUMNS ===\")\n",
        "print(list(df_preview.columns))\n",
        "assert TEXT_COL in df_preview.columns, f\"Column '{TEXT_COL}' not found. Available: {list(df_preview.columns)}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6) Create mapping/pipeline and index (as needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using existing index 'chat_elser_description_only' with 10 docs.\n"
          ]
        }
      ],
      "source": [
        "pipe = BertDescriptionElser(\n",
        "    es_url=ES_URL,\n",
        "    es_user=ES_USER,\n",
        "    es_pass=ES_PASS,\n",
        "    index_name=INDEX_NAME,\n",
        "    endpoint_id=ENDPOINT_ID,\n",
        "    description_col=TEXT_COL,\n",
        "    use_ml=(not BM25_ONLY),\n",
        ")\n",
        "ensure_indexed(pipe, str(fp), reindex=REINDEX)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7) One-shot search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SEARCH RESULTS for: 'BlueSky Airlines safety compliance' ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_score</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.704845</td>\n",
              "      <td>Mary Johnson joined BlueSky Airlines in 2018 a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     _score                                        Description\n",
              "0  4.704845  Mary Johnson joined BlueSky Airlines in 2018 a..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if ONE_SHOT_QUERY and str(ONE_SHOT_QUERY).strip():\n",
        "    print(f\"=== SEARCH RESULTS for: {ONE_SHOT_QUERY!r} ===\")\n",
        "    _hits = do_query(pipe, ONE_SHOT_QUERY, TEXT_COL, size=TOP_K, bm25_only=BM25_ONLY)\n",
        "else:\n",
        "    print(\"(Skipped)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8) Interactive search loop (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interactive mode. Type your query and press Enter.\n",
            "Commands: :quit to exit, :help for help.\n",
            "\n",
            "=== SEARCH RESULTS for: 'when does Robert Green born?' ===\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_score</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.520117</td>\n",
              "      <td>Robert Green, born December 9, 1975, serves as...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.090938</td>\n",
              "      <td>Sarah Thompson currently teaches environmental...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.090938</td>\n",
              "      <td>Miguel Santos, born July 11, 1991, is a logist...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.090938</td>\n",
              "      <td>Olivia Brown joined MedCare Hospital in 2015 a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.087962</td>\n",
              "      <td>Ahmed Ibrahim, a senior architect at GreenBuil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.087962</td>\n",
              "      <td>Li Wei, born on August 5, 1993, works for Sino...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.086546</td>\n",
              "      <td>Mary Johnson joined BlueSky Airlines in 2018 a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.086546</td>\n",
              "      <td>David Kim, born February 22, 1987, is employed...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.082558</td>\n",
              "      <td>John Doe, born on January 14, 1985, currently ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     _score                                        Description\n",
              "0  2.520117  Robert Green, born December 9, 1975, serves as...\n",
              "1  0.090938  Sarah Thompson currently teaches environmental...\n",
              "2  0.090938  Miguel Santos, born July 11, 1991, is a logist...\n",
              "3  0.090938  Olivia Brown joined MedCare Hospital in 2015 a...\n",
              "4  0.087962  Ahmed Ibrahim, a senior architect at GreenBuil...\n",
              "5  0.087962  Li Wei, born on August 5, 1993, works for Sino...\n",
              "6  0.086546  Mary Johnson joined BlueSky Airlines in 2018 a...\n",
              "7  0.086546  David Kim, born February 22, 1987, is employed...\n",
              "8  0.082558  John Doe, born on January 14, 1985, currently ..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Interactive mode. Type your query and press Enter.\")\n",
        "print(\"Commands: :quit to exit, :help for help.\")\n",
        "while True:\n",
        "    try:\n",
        "        q = input(\"query> \").strip()\n",
        "    except (EOFError, KeyboardInterrupt):\n",
        "        print(\"\\nExiting.\")\n",
        "        break\n",
        "    if not q:\n",
        "        continue\n",
        "    if q in {\":quit\", \":exit\"}:\n",
        "        print(\"Exiting.\")\n",
        "        break\n",
        "    if q in {\":help\", \"help\", \"?\"}:\n",
        "        print(\"Enter any text to search. Use :quit to exit.\")\n",
        "        continue\n",
        "    print(f\"\\n=== SEARCH RESULTS for: {q!r} ===\")\n",
        "    _hits = do_query(pipe, q, TEXT_COL, size=TOP_K, bm25_only=BM25_ONLY)\n",
        "    print(\"\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
