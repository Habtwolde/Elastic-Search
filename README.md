Oracle → Elasticsearch with Logstash & ELSER

Step 1 — Install Oracle Database Free (XE) on Windows and load sample data
1. Install Oracle Database Free (XE)

2. Test that Oracle is up

Open Command Prompt (or PowerShell) and run:

 ```   
  sqlplus system@localhost:1521/XEPDB1
 ```
3. Create an application user
At the SQL> prompt, run this (replace the password as you like):
```
ALTER USER system IDENTIFIED BY "NewStrongPassword#2025";

exit
```
Test new login
```
sqlplus system/NewStrongPassword#2025@localhost:1521/XEPDB1
```
4. Create a new user/schema

```
CREATE USER es_user IDENTIFIED BY "EsUserPass#2025"
  DEFAULT TABLESPACE users
  TEMPORARY TABLESPACE temp
  QUOTA UNLIMITED ON users;
```
5. Grant permissions
```
GRANT CONNECT, RESOURCE TO es_user;
GRANT CREATE SESSION, CREATE TABLE, CREATE VIEW TO es_us
```
6. Create sample table + seed data (and make it update-friendly)

Paste the following exactly into your SQL> session (you’re inside sqlplus as es_user):
```
CREATE TABLE docs (
  id          NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  title       VARCHAR2(400),
  body        CLOB,
  updated_at  TIMESTAMP DEFAULT SYSTIMESTAMP
);

CREATE INDEX docs_updated_at_idx ON docs (updated_at);
```
Insert a few rows into docs
```
INSERT INTO docs (title, body) VALUES (
  'Bekele records',
  'Kenenisa Bekele set multiple world records in long-distance running.'
);

INSERT INTO docs (title, body) VALUES (
  'USMNT Gold Cup',
  'U.S. men''s national team won the Gold Cup in a thrilling penalty shootout.'
);

INSERT INTO docs (title, body) VALUES (
  'Jamaica weather',
  'Heavy rainfall caused flooding in Jamaica and disrupted travel.'
);

COMMIT;
```
Run these, then 👉 You should see 3 rows:

```
SELECT id, title, SUBSTR(body,1,80) AS body_preview, updated_at
FROM docs
ORDER BY id;
```

Step 2 — Logstash JDBC pipeline (Docker)

1. Make folders for the Logstash build
```powershell

mkdir C:\Users\dell\oracle-to-es\pipeline -Force | Out-Null
mkdir C:\Users\dell\oracle-to-es\drivers  -Force | Out-Null
```

2 Put the Oracle JDBC driver in place

Download Oracle’s JDBC JAR (e.g., ojdbc8.jar) requires accepting Oracle’s license, so download it manually in your browser from Oracle and save it to:

```vbnet
C:\Users\dell\oracle-to-es\drivers\ojdbc8.jar
```

3 Create the Logstash.config pipeline in C:\Users\dell\oracle-to-es\pipeline and paste this 

```
input {
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/drivers/ojdbc8.jar"
    jdbc_driver_class   => "Java::oracle.jdbc.OracleDriver"

    # Connect from container → Windows host Oracle XE
    jdbc_connection_string => "jdbc:oracle:thin:@//host.docker.internal:1521/XEPDB1"
    jdbc_user     => "es_user"
    jdbc_password => "EsUserPass#2025"

    jdbc_validate_connection => true
    jdbc_fetch_size => 200
    jdbc_default_timezone => "UTC"

    # Run every minute (adjust later)
    schedule => "* * * * *"

    # Track by timestamp column
    use_column_value => false
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    record_last_run => true
    last_run_metadata_path => "/usr/share/logstash/.logstash_jdbc_last_run"

    # Important: handle first run via NVL(:sql_last_value, ...) so it loads everything once,
    # then only rows with updated_at > last checkpoint.
    statement => "
      SELECT
        id,
        title,
        body,
        updated_at
      FROM docs
      WHERE updated_at > NVL(:sql_last_value, TO_TIMESTAMP('1970-01-01','YYYY-MM-DD'))
      ORDER BY updated_at ASC
    "
  }
}

filter {
  # Build a single text field for ELSER to embed (title + newline + body)
  mutate {
    # guard against nulls; Logstash treats missing as ''
    add_field => { "content" => "%{title}\n%{body}" }
  }

  # Stable ES _id from Oracle id
  mutate { update => { "[@metadata][doc_id]" => "%{id}" } }
}

output {
  elasticsearch {
    hosts    => ["http://elasticsearch:9200"]
    user     => "elastic"
    password => "changeme"

    index    => "oracle_elser_index"
    pipeline => "elser_v2_pipeline"   # ⇐ runs ELSER inference to produce ml.tokens

    document_id   => "%{[@metadata][doc_id]}"
    doc_as_upsert => true
    action        => "index"
  }

  # Useful while testing
  stdout { codec => json_lines }
}

```
4 Create a Dockerfile Dockerfile Logstash in C:\Users\dell\oracle-to-es In Powwershell

```powershell
@'
FROM docker.elastic.co/logstash/logstash:8.14.3

# Drivers + pipeline
COPY drivers/ojdbc8.jar /usr/share/logstash/drivers/ojdbc8.jar
COPY pipeline/logstash.conf /usr/share/logstash/pipeline/logstash.conf

'@ | Set-Content -Encoding ASCII C:\Users\dell\oracle-to-es\Dockerfile

```
Or paste this in to Docker file
```

FROM docker.elastic.co/logstash/logstash:8.14.3

# Drivers + pipeline
COPY drivers/ojdbc8.jar /usr/share/logstash/drivers/ojdbc8.jar
COPY pipeline/logstash.conf /usr/share/logstash/pipeline/logstash.conf

```



5 Add Logstash to your existing docker-compose.yml (Replace it)

```
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.3
    container_name: es01
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true
      - xpack.license.self_generated.type=trial
      - xpack.ml.enabled=true
      - ES_JAVA_OPTS=-Xms2g -Xmx2g
      - ELASTIC_PASSWORD=changeme
      - xpack.ml.model_repository=file:///usr/share/elasticsearch/config/models
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      # Map the *subfolder* so it becomes the repo root in the container
      - type: bind
        source: "C:\\ml-models\\.elser_model_2_linux-x86_64"
        target: /usr/share/elasticsearch/config/models
        read_only: true

  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.3
    container_name: kb01
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOSTS=["http://elasticsearch:9200"]
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=kibana_password123
      - SERVER_PUBLICBASEURL=http://localhost:5601
      - SERVER_HOST=0.0.0.0
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
    ports:
      - "5601:5601"
  logstash:
    build:
      context: "C:\\Users\\dell\\oracle-to-es"
    container_name: ls01
    depends_on:
      - elasticsearch
    environment:
      LS_JAVA_OPTS: "-Xms1g -Xmx1g"
```
6. build and run Logstash

```
cd C:\Users\dell\elasticsearch-docker

# (Optional) Validate YAML
docker compose config

# Rebuild without cache to ensure it picks up the new Dockerfile
docker compose build --no-cache logstash

# Start the service
docker compose up -d logstash

# Tail logs to verify JDBC driver is loaded and pipeline is running
docker logs -f ls01
```
Step 3 Verify docs landed in Elasticsearch

Open Kibana → Dev Tools and run:
```
GET oracle_elser_index/_count
GET oracle_elser_index/_search
{
  "size": 3,
  "sort": [{ "updated_at": "desc" }],
  "_source": ["id","title","content","ml.tokens","updated_at"]
}

GET _ml/trained_models/.elser_model_2_linux-x86_64/_stats
GET _ingest/pipeline/elser_v2_pipeline
```
2 Ensure the ELSER ingest pipeline exists

```
PUT _ingest/pipeline/elser_v2_pipeline
{
  "processors": [
    {
      "inference": {
        "model_id": ".elser_model_2_linux-x86_64",
        "input_output": [
          { "input_field": "content", "output_field": "ml.tokens" }
        ],
        "inference_config": { "text_expansion": {} }
      }
    }
  ]
}
```

3 Create mapping (if index missing)

If oracle_elser_index didn’t exist, create it before Logstash writes (or after deleting it if you want a clean start):

```
PUT oracle_elser_index
{
  "mappings": {
    "properties": {
      "id":         { "type": "long" },
      "title":      { "type": "text" },
      "body":       { "type": "text" },
      "content":    { "type": "text" },
      "updated_at": { "type": "date" },
      "ml": {
        "properties": {
          "tokens": { "type": "rank_features" }
        }
      }
    }
  }
}
```
If the index already exists with the wrong mapping, delete it first:

```
DELETE oracle_elser_index
```
4 Semantic search test (ELSER)

Once documents exist and ml.tokens is populated via the pipeline, test:

```
POST oracle_elser_index/_search
{
  "size": 5,
  "query": {
    "text_expansion": {
      "ml.tokens": {
        "model_id": ".elser_model_2_linux-x86_64",
        "model_text": "who set records in long distance running?"
      }


    }
  },
  "_source": ["title","content","updated_at"]
}
```

Create a Python script ingest_to_es_elser.py
```
pip install elasticsearch pandas openpyxl python-dateutil pymupdf


```

```
import argparse, json, math, os, sys, time, uuid
from pathlib import Path
from datetime import datetime

import pandas as pd
from dateutil import parser as dtparser
import fitz  # PyMuPDF
from elasticsearch import Elasticsearch, helpers

# ---- Common ES/ELSER config ----
ES_URL   = os.environ.get("ES_URL", "http://localhost:9200")
ES_USER  = os.environ.get("ES_USER", "elastic")
ES_PASS  = os.environ.get("ES_PASS", "changeme")

MODEL_ID     = ".elser_model_2_linux-x86_64"
PIPELINE_ID  = "elser_v2_pipeline"
TOKENS_FIELD = "ml.tokens"

# ---- Connect ----
ES = Elasticsearch(ES_URL, basic_auth=(ES_USER, ES_PASS), request_timeout=120)

def wait_es(timeout_s=60):
    deadline = time.time() + timeout_s
    while time.time() < deadline:
        try:
            ES.info()
            return
        except Exception:
            time.sleep(1)
    raise RuntimeError("Elasticsearch not responding")

def ensure_model_started():
    """Verify the ELSER model is deployed; start if not."""
    try:
        stats = ES.ml.get_trained_models_stats(model_id=MODEL_ID)
        tms = stats.get("trained_model_stats", [])
        if tms:
            dstats = tms[0].get("deployment_stats") or {}
            if dstats.get("state") == "started":
                return
    except Exception:
        pass
    try:
        ES.ml.start_trained_model_deployment(
            model_id=MODEL_ID,
            number_of_allocations=1,
            threads_per_allocation=1,
            queue_capacity=1024,
        )
    except Exception:
        # no-op if already started / transient
        pass

def ensure_pipeline():
    """Create/update the ELSER pipeline (idempotent)."""
    pipeline = {
        "processors": [
            {
                "inference": {
                    "model_id": MODEL_ID,
                    "input_output": [
                        {"input_field": "content", "output_field": TOKENS_FIELD}
                    ],
                    "inference_config": {"text_expansion": {}}
                }
            }
        ]
    }
    ES.ingest.put_pipeline(id=PIPELINE_ID, processors=pipeline["processors"])

def ensure_index(index: str, with_extra_fields=None):
    """Create index with ELSER mapping if missing."""
    if ES.indices.exists(index=index):
        return
    props = {
        "content": {"type": "text"},
        "ml": {"properties": {"tokens": {"type": "rank_features"}}}
    }
    if with_extra_fields:
        props.update(with_extra_fields)
    ES.indices.create(index=index, body={"mappings": {"properties": props}})

# ---------- helpers ----------
def to_dt(v):
    if pd.isna(v):
        return None
    if isinstance(v, datetime):
        return v
    try:
        return dtparser.parse(str(v))
    except Exception:
        return None

# ---------- Excel / CSV ingestion ----------
def ingest_table(
    index: str,
    file_path: Path,
    sheet=None,
    *,
    id_col="id",
    title_col="title",
    body_col="body",
    updated_col="updated_at",
    country_col="Country",
    date_col="Date",
    time_col="Time (HH:MM:SS)",
    position_col="Position",
    batch=1000,
):
    # read df
    if file_path.suffix.lower() == ".xlsx":
        sheet_name = None
        if sheet is not None:
            try:
                sheet_name = int(sheet)
            except ValueError:
                sheet_name = sheet
        df = pd.read_excel(file_path, sheet_name=sheet_name, engine="openpyxl")
    elif file_path.suffix.lower() == ".csv":
        df = pd.read_csv(file_path)
    else:
        raise SystemExit("Unsupported tabular format. Use .xlsx or .csv")

    # case-insensitive column mapping
    cols = {c.lower().strip(): c for c in df.columns}
    def col(name): return cols.get(name.lower(), name)

    id_col       = col(id_col)
    title_col    = col(title_col)   # Name
    body_col     = col(body_col)    # Event (by your CLI)
    updated_col  = col(updated_col)
    country_col  = col(country_col)
    date_col     = col(date_col)
    time_col     = col(time_col)
    position_col = col(position_col)

    missing = [c for c in [id_col, title_col] if c not in df.columns]
    if missing:
        print(f"ERROR: Missing required columns: {missing}", file=sys.stderr)
        sys.exit(2)

    actions = []
    for _, row in df.iterrows():
        rid      = row.get(id_col)
        title    = row.get(title_col)        # Name
        event    = row.get(body_col) if body_col in df.columns else None
        country  = row.get(country_col) if country_col in df.columns else None
        date_val = row.get(date_col) if date_col in df.columns else None
        time_val = row.get(time_col) if time_col in df.columns else None
        pos_val  = row.get(position_col) if position_col in df.columns else None
        updated  = to_dt(row.get(updated_col)) if updated_col in df.columns else None

        # Normalize date to ISO string if present
        date_iso = None
        if date_val is not None:
            try:
                date_iso = to_dt(date_val).date().isoformat()
            except Exception:
                pass

        # Rich content text for ELSER
        parts = []
        if title:    parts.append(str(title))
        if event:    parts.append(f"Event: {event}")
        if country:  parts.append(f"Country: {country}")
        if date_iso: parts.append(f"Date: {date_iso}")
        if time_val is not None and str(time_val).strip() != "":
            parts.append(f"Time: {time_val}")
        if pos_val is not None and str(pos_val).strip() != "":
            parts.append(f"Position: {pos_val}")

        content = ". ".join(parts) if parts else (str(title) or "")

        # Document to index (kept structured fields too)
        doc = {
            "id":       rid,
            "title":    title,          # Name
            "body":     event,          # Event (compat)
            "event":    event,
            "country":  country,
            "date":     date_iso,
            "time_raw": str(time_val) if time_val is not None else None,
            "position": int(pos_val) if str(pos_val).isdigit() else None,
            "content":  content,
        }
        if updated is not None:
            doc["updated_at"] = updated.isoformat()

        actions.append({
            "_op_type": "index",
            "_index": index,
            "_id": str(rid) if rid is not None else None,
            "pipeline": PIPELINE_ID,
            "_source": doc
        })

    if not actions:
        print("No rows to index.")
        return

    print(f"Indexing {len(actions)} rows from '{file_path.name}' → '{index}' via '{PIPELINE_ID}'...")
    success, fail = helpers.bulk(ES, actions, stats_only=True, chunk_size=batch, request_timeout=120)
    ES.indices.refresh(index=index)
    print(f"Done. success={success}, failed={fail}")

# ---------- PDF ingestion ----------
def chunk_text(text: str, chunk_size: int = 1200, overlap: int = 200):
    text = (text or "").strip()
    if not text:
        return []
    chunks = []
    n = len(text)
    start = 0
    while start < n:
        end = min(n, start + chunk_size)
        slice_ = text[start:end]
        if end < n:
            last_dot = slice_.rfind(".")
            if last_dot > int(chunk_size * 0.6):
                end = start + last_dot + 1
                slice_ = text[start:end]
        chunk = slice_.strip()
        if chunk:
            chunks.append(chunk)
        start = end if end >= n else end - overlap
    return chunks

def extract_pdf(path: Path, max_pages: int | None = None):
    with fitz.open(path) as doc:
        total = doc.page_count
        limit = total if max_pages is None else min(max_pages, total)
        for i in range(limit):
            page = doc.load_page(i)
            txt = page.get_text("text") or ""
            if txt.strip():
                yield (i + 1, txt)

def bulk_line(index: str, pdf_path: Path, page: int, chunk_id: int, content: str):
    meta = {"index": {"_index": index, "_id": str(uuid.uuid4())}}
    src = {"path": str(pdf_path), "page": page, "chunk": chunk_id, "content": content}
    return json.dumps(meta) + "\n" + json.dumps(src, ensure_ascii=False) + "\n"

def ingest_pdf(index: str, input_path: Path, chunk_size=1200, overlap=200, max_pages=None, batch=500):
    pdfs = [input_path] if input_path.is_file() else [p for p in input_path.rglob("*.pdf")]
    if not pdfs:
        print("No PDFs found.")
        return
    print(f"Indexing {len(pdfs)} PDF(s) → '{index}' via '{PIPELINE_ID}'...")
    buf, sent = [], 0
    for pdf in pdfs:
        try:
            for page_num, text in extract_pdf(pdf, max_pages=max_pages):
                for j, chunk in enumerate(chunk_text(text, chunk_size, overlap), start=1):
                    buf.append(bulk_line(index, pdf, page_num, j, chunk))
                    if len(buf) >= batch * 2:  # 2 lines per action
                        resp = ES.bulk(body="".join(buf), pipeline=PIPELINE_ID)
                        if resp.get("errors"):
                            first = next((it for it in resp["items"] if "error" in list(it.values())[0]), None)
                            print("Bulk errors; first:", first)
                        sent += len(buf) // 2
                        buf = []
        except Exception as e:
            print(f"Error processing {pdf}: {e}")
    if buf:
        resp = ES.bulk(body="".join(buf), pipeline=PIPELINE_ID)
        if resp.get("errors"):
            first = next((it for it in resp["items"] if "error" in list(it.values())[0]), None)
            print("Bulk errors; first:", first)
        sent += len(buf) // 2
    ES.indices.refresh(index=index)
    print(f"Done. Indexed ~{int(sent)} chunks.")

# ---------- Query ----------
def semantic_search(index: str, query: str, size: int = 5):
    try:
        cnt = ES.count(index=index).get("count", 0)
        if cnt == 0:
            print(f"(Index '{index}' has 0 docs — nothing to search yet.)")
    except Exception:
        pass

    body = {
        "size": size,
        "query": {
            "text_expansion": {
                TOKENS_FIELD: {
                    "model_id": MODEL_ID,
                    "model_text": query
                }
            }
        },
        "_source": ["id","title","event","country","content","path","page","updated_at"]
    }
    res = ES.search(index=index, body=body)
    print(f"\nQuery: {query}")
    hits = res.get("hits", {}).get("hits", [])
    if not hits:
        print("- no hits -")
        return res
    for h in hits:
        src = h["_source"]
        title = src.get("title") or Path(src.get("path","")).name or ""
        preview = (src.get("content") or "")[:160].replace("\n"," ")
        print(f"- score={h['_score']:.3f}  title={title}  page={src.get('page')}  preview={preview}")
    return res

# ---------- CLI ----------
def main():
    ap = argparse.ArgumentParser(
        description="Ingest Excel/CSV/PDF into Elasticsearch with ELSER pipeline (ml.tokens) and optionally query."
    )
    ap.add_argument("--index", required=True, help="Target index (e.g., oracle_elser_index or pdf_elser_index)")
    ap.add_argument("--file", required=True, help="Path to .xlsx / .csv / .pdf or a folder of PDFs")
    ap.add_argument("--sheet", default=None, help="Excel sheet name or index (for .xlsx)")
    ap.add_argument("--id-col", default="id")
    ap.add_argument("--title-col", default="title")
    ap.add_argument("--body-col", default="body")
    ap.add_argument("--updated-col", default="updated_at")
    ap.add_argument("--country-col",  default="Country")
    ap.add_argument("--date-col",     default="Date")
    ap.add_argument("--time-col",     default="Time (HH:MM:SS)")
    ap.add_argument("--position-col", default="Position")
    ap.add_argument("--batch", type=int, default=1000)
    ap.add_argument("--chunk-size", type=int, default=1200, help="PDF chars per chunk")
    ap.add_argument("--overlap", type=int, default=200, help="PDF chunk overlap")
    ap.add_argument("--max-pages", type=int, default=None, help="PDF page limit per doc")
    ap.add_argument("--query", default=None, help="Optional: run a semantic question after ingest")
    ap.add_argument("--topk", type=int, default=5, help="Hits to return for --query")

    args = ap.parse_args()

    wait_es()
    ensure_model_started()
    ensure_pipeline()

    p = Path(args.file)

    if p.suffix.lower() in [".xlsx", ".csv"]:
        # table-like index (id/title/body/content/updated_at + extras)
        ensure_index(args.index, with_extra_fields={
            "id":        {"type": "keyword"},
            "title":     {"type": "text"},
            "body":      {"type": "text"},
            "country":   {"type": "keyword"},
            "event":     {"type": "keyword"},
            "date":      {"type": "date"},
            "time_raw":  {"type": "keyword"},
            "position":  {"type": "integer"},
            "updated_at":{"type": "date"}
        })
        ingest_table(
            index=args.index,
            file_path=p,
            sheet=args.sheet,
            id_col=args.id_col,
            title_col=args.title_col,
            body_col=args.body_col,
            updated_col=args.updated_col,
            country_col=args.country_col,
            date_col=args.date_col,
            time_col=args.time_col,
            position_col=args.position_col,
            batch=args.batch
        )
    else:
        # PDF mode: support single file or folder
        ensure_index(args.index, with_extra_fields={
            "path": {"type": "keyword"},
            "page": {"type": "integer"},
            "chunk": {"type": "integer"},
        })
        ingest_pdf(
            index=args.index,
            input_path=p,
            chunk_size=args.chunk_size,
            overlap=args.overlap,
            max_pages=args.max_pages,
            batch=args.batch
        )

    if args.query:
        semantic_search(args.index, args.query, size=args.topk)

if __name__ == "__main__":
    main()

```
Start Elasticsearch with ELSER model available (self-managed Docker or Elastic Cloud).
Then configure:

```
PUT _ingest/pipeline/elser_v2_pipeline
{
  "processors": [
    {
      "inference": {
        "model_id": ".elser_model_2_linux-x86_64",
        "input_output": [
          { "input_field": "content", "output_field": "ml.tokens" }
        ],
        "inference_config": { "text_expansion": {} }
      }
    }
  ]
}


```
Excel / CSV Ingest Example

```
python ingest_or_query_elser.py --mode excel --index excel_elser_index `
  --file "C:\Users\dell\elser-python\long_distance_runners_record.xlsx" `
  --sheet Sheet1 `
  --id-col "Runner ID" --title-col "Name" --body-col "Event" `
  --query "who set records in long distance running?"

```

```
curl -u elastic:changeme -H 'Content-Type: application/json' \
  -X POST http://localhost:9200/_security/user/kibana_system/_password \
  -d '{"password":"kibana_password123"}'
```

New .yml

```
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.3
    container_name: es01
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true
      - xpack.ml.enabled=true
      - xpack.license.self_generated.type=trial
      - ES_JAVA_OPTS=-Xms2g -Xmx2g
      - ELASTIC_PASSWORD=changeme
      - xpack.ml.model_repository=file:///usr/share/elasticsearch/config/models
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      # ✅ make sure this path exists on Windows
      - type: bind
        source: "C:\\ml-models\\.elser_model_2_linux-x86_64"
        target: /usr/share/elasticsearch/config/models
        read_only: true

  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.3
    container_name: kb01
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOSTS=["http://elasticsearch:9200"]
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=kibana_password123
    ports:
      - "5601:5601"


```
