Oracle → Elasticsearch with Logstash & ELSER

Step 1 — Install Oracle Database Free (XE) on Windows and load sample data
1. Install Oracle Database Free (XE)

2. Test that Oracle is up

Open Command Prompt (or PowerShell) and run:

 ```   
  sqlplus system@localhost:1521/XEPDB1
 ```
3. Create an application user
At the SQL> prompt, run this (replace the password as you like):
```
ALTER USER system IDENTIFIED BY "NewStrongPassword#2025";

exit
```
Test new login
```
sqlplus system/NewStrongPassword#2025@localhost:1521/XEPDB1
```
4. Create a new user/schema

```
CREATE USER es_user IDENTIFIED BY "EsUserPass#2025"
  DEFAULT TABLESPACE users
  TEMPORARY TABLESPACE temp
  QUOTA UNLIMITED ON users;
```
5. Grant permissions
```
GRANT CONNECT, RESOURCE TO es_user;
GRANT CREATE SESSION, CREATE TABLE, CREATE VIEW TO es_us
```
6. Create sample table + seed data (and make it update-friendly)

Paste the following exactly into your SQL> session (you’re inside sqlplus as es_user):
```
CREATE TABLE docs (
  id          NUMBER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  title       VARCHAR2(400),
  body        CLOB,
  updated_at  TIMESTAMP DEFAULT SYSTIMESTAMP
);

CREATE INDEX docs_updated_at_idx ON docs (updated_at);
```
Insert a few rows into docs
```
INSERT INTO docs (title, body) VALUES (
  'Bekele records',
  'Kenenisa Bekele set multiple world records in long-distance running.'
);

INSERT INTO docs (title, body) VALUES (
  'USMNT Gold Cup',
  'U.S. men''s national team won the Gold Cup in a thrilling penalty shootout.'
);

INSERT INTO docs (title, body) VALUES (
  'Jamaica weather',
  'Heavy rainfall caused flooding in Jamaica and disrupted travel.'
);

COMMIT;
```
Run these, then 👉 You should see 3 rows:

```
SELECT id, title, SUBSTR(body,1,80) AS body_preview, updated_at
FROM docs
ORDER BY id;
```

Step 2 — Logstash JDBC pipeline (Docker)

1. Make folders for the Logstash build
```powershell

mkdir C:\Users\dell\oracle-to-es\pipeline -Force | Out-Null
mkdir C:\Users\dell\oracle-to-es\drivers  -Force | Out-Null
```

2 Put the Oracle JDBC driver in place

Download Oracle’s JDBC JAR (e.g., ojdbc8.jar) requires accepting Oracle’s license, so download it manually in your browser from Oracle and save it to:

```vbnet
C:\Users\dell\oracle-to-es\drivers\ojdbc8.jar
```

3 Create the Logstash pipeline config file logstash.config file in C:\Users\dell\oracle-to-es\pipeline and paste this 

```
input {
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/drivers/ojdbc8.jar"
    jdbc_driver_class   => "Java::oracle.jdbc.OracleDriver"

    # Connect from container → Windows host Oracle XE
    jdbc_connection_string => "jdbc:oracle:thin:@//host.docker.internal:1521/XEPDB1"
    jdbc_user     => "es_user"
    jdbc_password => "EsUserPass#2025"

    jdbc_validate_connection => true
    jdbc_fetch_size => 200
    jdbc_default_timezone => "UTC"

    # Run every minute (adjust later)
    schedule => "* * * * *"

    # Track by timestamp column
    use_column_value => false
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    record_last_run => true
    last_run_metadata_path => "/usr/share/logstash/.logstash_jdbc_last_run"

    # Important: handle first run via NVL(:sql_last_value, ...) so it loads everything once,
    # then only rows with updated_at > last checkpoint.
    statement => "
      SELECT
        id,
        title,
        body,
        updated_at
      FROM docs
      WHERE updated_at > NVL(:sql_last_value, TO_TIMESTAMP('1970-01-01','YYYY-MM-DD'))
      ORDER BY updated_at ASC
    "
  }
}

filter {
  # Build a single text field for ELSER to embed (title + newline + body)
  mutate {
    # guard against nulls; Logstash treats missing as ''
    add_field => { "content" => "%{title}\n%{body}" }
  }

  # Stable ES _id from Oracle id
  mutate { update => { "[@metadata][doc_id]" => "%{id}" } }
}

output {
  elasticsearch {
    hosts    => ["http://elasticsearch:9200"]
    user     => "elastic"
    password => "changeme"

    index    => "oracle_elser_index"
    pipeline => "elser_v2_pipeline"   # ⇐ runs ELSER inference to produce ml.tokens

    document_id   => "%{[@metadata][doc_id]}"
    doc_as_upsert => true
    action        => "index"
  }

  # Useful while testing
  stdout { codec => json_lines }
}

```
4 Create a Dockerfile Dockerfile Logstash in C:\Users\dell\oracle-to-es 
In Powwershell

```powershell
@'
FROM docker.elastic.co/logstash/logstash:8.14.3

# Drivers + pipeline
COPY drivers/ojdbc8.jar /usr/share/logstash/drivers/ojdbc8.jar
COPY pipeline/logstash.conf /usr/share/logstash/pipeline/logstash.conf

'@ | Set-Content -Encoding ASCII C:\Users\dell\oracle-to-es\Dockerfile

```
Or paste this
```

FROM docker.elastic.co/logstash/logstash:8.14.3

# Drivers + pipeline
COPY drivers/ojdbc8.jar /usr/share/logstash/drivers/ojdbc8.jar
COPY pipeline/logstash.conf /usr/share/logstash/pipeline/logstash.conf

```



5 Add Logstash to your existing docker-compose.yml (Replace it)

```
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.14.3
    container_name: es01
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=true
      - xpack.license.self_generated.type=trial
      - xpack.ml.enabled=true
      - ES_JAVA_OPTS=-Xms2g -Xmx2g
      - ELASTIC_PASSWORD=changeme
      - xpack.ml.model_repository=file:///usr/share/elasticsearch/config/models
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      # Map the *subfolder* so it becomes the repo root in the container
      - type: bind
        source: "C:\\ml-models\\.elser_model_2_linux-x86_64"
        target: /usr/share/elasticsearch/config/models
        read_only: true

  kibana:
    image: docker.elastic.co/kibana/kibana:8.14.3
    container_name: kb01
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOSTS=["http://elasticsearch:9200"]
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=kibana_password123
      - SERVER_PUBLICBASEURL=http://localhost:5601
      - SERVER_HOST=0.0.0.0
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
    ports:
      - "5601:5601"
  logstash:
    build:
      context: "C:\\Users\\dell\\oracle-to-es"
    container_name: ls01
    depends_on:
      - elasticsearch
    environment:
      LS_JAVA_OPTS: "-Xms1g -Xmx1g"
```
6. build and run Logstash

```
cd C:\Users\dell\elasticsearch-docker

# (Optional) Validate YAML
docker compose config

# Rebuild without cache to ensure it picks up the new Dockerfile
docker compose build --no-cache logstash

# Start the service
docker compose up -d logstash

# Tail logs to verify JDBC driver is loaded and pipeline is running
docker logs -f ls01
```
Step 3 Verify docs landed in Elasticsearch

Open Kibana → Dev Tools and run:
```
GET oracle_elser_index/_count

GET oracle_elser_index/_search
{
  "size": 5,
  "_source": ["id","title","content","ml.tokens","updated_at"]
}
```
2 Ensure the ELSER ingest pipeline exists

```
PUT _ingest/pipeline/elser_v2_pipeline
{
  "processors": [
    {
      "inference": {
        "model_id": ".elser_model_2_linux-x86_64",
        "input_output": [
          { "input_field": "content", "output_field": "ml.tokens" }
        ],
        "inference_config": { "text_expansion": {} }
      }
    }
  ]
}
```

3 Create mapping (if index missing)

If oracle_elser_index didn’t exist, create it before Logstash writes (or after deleting it if you want a clean start):

```
PUT oracle_elser_index
{
  "mappings": {
    "properties": {
      "id":         { "type": "long" },
      "title":      { "type": "text" },
      "body":       { "type": "text" },
      "content":    { "type": "text" },
      "updated_at": { "type": "date" },
      "ml": {
        "properties": {
          "tokens": { "type": "rank_features" }
        }
      }
    }
  }
}
```
If the index already exists with the wrong mapping, delete it first:

```
DELETE oracle_elser_index
```
4 Semantic search test (ELSER)

Once documents exist and ml.tokens is populated via the pipeline, test:

```
POST oracle_elser_index/_search
{
  "size": 5,
  "query": {
    "text_expansion": {
      "ml.tokens": {
        "model_id": ".elser_model_2_linux-x86_64",
        "model_text": "who set records in long distance running?"
      }


    }
  },
  "_source": ["title","content","updated_at"]
}
```

```
curl -u elastic:changeme -H 'Content-Type: application/json' \
  -X POST http://localhost:9200/_security/user/kibana_system/_password \
  -d '{"password":"kibana_password123"}'
```
